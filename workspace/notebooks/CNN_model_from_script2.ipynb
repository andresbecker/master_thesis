{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "The objective of this notebook is train and evaluate a given model specified in the parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "    \n",
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "\n",
    "# Save parameter file path\n",
    "p['parameters_file_path'] = PARAMETERS_FILE\n",
    "p.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging configuration\n",
    "import logging\n",
    "log_file_path = p['log_file_name']\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w', \n",
    "    level=getattr(logging, p['log_level'])\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing parameters with default values\n",
    "if not 'conv_reg' in p.keys():\n",
    "    p['conv_reg'] = [0,0]\n",
    "if not 'dense_reg' in p.keys():\n",
    "    p['dense_reg'] = [0,0]\n",
    "    \n",
    "if not 'verbose_level' in p.keys():\n",
    "    p['verbose_level'] = 2\n",
    "    \n",
    "if not 'pre_training' in p.keys():\n",
    "    p['pre_training'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Dataset:\\n\\t{}'.format(p['tf_ds_name'])\n",
    "msg += '\\n\\nData Augmentation:'\n",
    "msg += '\\n\\tRandom Flipping: {}\\n\\tRandom 90deg Rotations: {}'.format(p['random_horizontal_flipping'],p['random_90deg_rotations'])\n",
    "msg += '\\n\\tRandom centerd zoom: {}'.format(p['random_CenterZoom'])\n",
    "msg += '\\n\\nModel:'\n",
    "msg += '\\n\\tArchitecture: {}'.format(p['model_name'])\n",
    "msg += '\\n\\tpre_training: {}'.format(p['pre_training'])\n",
    "msg += '\\n\\tConv layers regularization ([l1, l2]): {}'.format(p['conv_reg'])\n",
    "msg += '\\n\\tDense layers regularization ([l1, l2]): {}'.format(p['dense_reg'])\n",
    "msg += '\\n\\tLoss function: {}'.format(p['loss'])\n",
    "msg += '\\n\\tLearning rate: {}'.format(p['learning_rate'])\n",
    "msg += '\\n\\tEpochs: {}\\n\\n'.format(p['number_of_epochs'])\n",
    "logging.info(msg)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external libraries path\n",
    "EXTERNAL_LIBS_PATH = p['external_libs_path']\n",
    "if not os.path.exists(EXTERNAL_LIBS_PATH):\n",
    "    msg = 'External library path {} does not exist!'.format(EXTERNAL_LIBS_PATH)\n",
    "    logging.error(msg)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    msg='EXTERNAL_LIBS_PATH: {}'.format(EXTERNAL_LIBS_PATH)\n",
    "    print(msg)\n",
    "    logging.info(msg)\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, EXTERNAL_LIBS_PATH)\n",
    "# Load external libraries\n",
    "from Models_V2 import Predef_models as predef_models\n",
    "from Utils import Tee_Logger as Tee_Logger\n",
    "#from Utils import lr_schedule_Callback\n",
    "#from Utils import save_best_model_Callback\n",
    "from Utils import save_best_model_base_on_CMA_Callback\n",
    "from Utils import evaluate_model\n",
    "import Utils as utils\n",
    "import Data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dirs where model output will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to avoid cleaning (deleting) model dir, then uncomment the next line:\n",
    "#p['clean_model_dir'] = 0\n",
    "\n",
    "base_path, model_path, checkpoints_path = utils.create_model_dirs(parameters=p)\n",
    "\n",
    "msg = 'Base path:\\n{}'.format(base_path)\n",
    "msg += '\\nModel path:\\n{}'.format(model_path)\n",
    "msg += '\\nCheckpoints path:\\n{}'.format(checkpoints_path)\n",
    "logging.info(msg)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf to ignore GPU\n",
    "if p['disable_gpu']:\n",
    "    msg = \"Cuda devices (GPUs) disabled\"\n",
    "    logging.info(msg)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "msg = 'Physical GPU devises:\\n{}'.format(physical_devices)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "#restrict GPU mem\n",
    "if p['set_memory_growth']:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        msg = 'GPU Memory limited!'\n",
    "    except:\n",
    "        msg = 'It was not possible to limit GPU memory'\n",
    "        \n",
    "    logging.info(msg)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessing parameters and information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data path\n",
    "pp_path = p['pp_path']\n",
    "\n",
    "with open(os.path.join(pp_path, 'params.json')) as file:\n",
    "    pp_params = json.load(file)\n",
    "msg = 'Loaded data preprocessing parameters from:\\n{}'.format(file)\n",
    "logging.info(msg)\n",
    "seed = pp_params['seed']\n",
    "\n",
    "# Load Channels file\n",
    "with open(os.path.join(pp_path, 'channels.csv')) as file:\n",
    "    channels = pd.read_csv(file)\n",
    "msg = 'Loaded channels file from:\\n{}'.format(file)\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_channels = p['input_channels']\n",
    "msg = 'Selected input channels:\\n{}'.format(selected_channels)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "# Get selected channel ids\n",
    "input_ids = np.array(channels.set_index(['name']).loc[selected_channels].channel_id.values)\n",
    "msg = 'Corresponding input channel ids:\\n{}'.format(input_ids)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where tf datasets are\n",
    "dataset, metadata = tfds.load(\n",
    "    name=p['tf_ds_name'], \n",
    "    data_dir=p['local_tf_datasets'], \n",
    "    # If False, returns a dictionary with all the features\n",
    "    as_supervised=True, \n",
    "    shuffle_files=p['shuffle_files'],\n",
    "    with_info=True)\n",
    "msg = 'Tensorflow dataset {} loaded from:\\n{}'.format(p['tf_ds_name'], p['local_tf_datasets'])\n",
    "logging.info(msg)\n",
    "\n",
    "# Load the splits\n",
    "train_data, val_data, test_data = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "Before training the network, we discriminate some channels, apply some linear transformations (90deg rotations and horizontal flipping) to augment the **Training** dataset, create the batches and shuffle them. Also, we perform other operations to improve performance.\n",
    "\n",
    "**Tune performance**<br>\n",
    "tf.data.Dataset.prefetch overlaps data preprocessing and model execution while training.\n",
    "It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "\n",
    "**Shuffling**<br>\n",
    "dataset.shuffle() Randomly shuffles the elements of this dataset.\n",
    "This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "**reshuffle_each_iteration** controls whether the shuffle order should be different for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into one image and a random transformation (random rotation+random horizontal flippig):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image from the training dataset\n",
    "image, target = next(iter(train_data))\n",
    "# Visualize the original vs. random flipping and rotations\n",
    "plt_size=np.array([5,5])\n",
    "\n",
    "plt.figure(figsize=(plt_size[0],plt_size[1]))\n",
    "Data_augmentation.visualize_tensor_cell_image(image, 'Original Cell')\n",
    "\n",
    "if p['random_horizontal_flipping'] | p['random_90deg_rotations'] | p['random_CenterZoom']:\n",
    "    plt.figure(figsize=(4*plt_size[0],plt_size[1]))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1,4,i+1)\n",
    "        img, _ = Data_augmentation.augment(image, target, p, input_ids, metadata)\n",
    "        Data_augmentation.visualize_tensor_cell_image(img, 'Augmented Cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets for training the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = p['BATCH_SIZE']\n",
    "buffer_size = 512\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# the lambda function is to give more arguments to the map function\n",
    "\n",
    "train_data = train_data.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
    "train_data = train_data.map(lambda image, target: Data_augmentation.augment(image, target, p, input_ids, metadata), num_parallel_calls=AUTOTUNE)\n",
    "train_data = train_data.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_data = val_data.map(lambda image, target: Data_augmentation.filter_channels(image, target, input_ids, metadata), num_parallel_calls=AUTOTUNE)\n",
    "val_data = val_data.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "test_data = test_data.map(lambda image, target: Data_augmentation.filter_channels(image, target, input_ids, metadata), num_parallel_calls=AUTOTUNE)\n",
    "test_data = test_data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Models are selected from a group of predefined models in the class `Predef_models` (in `Models.py`). The name of the selected model is specified in the parameter `p['model_method']`.\n",
    "\n",
    "First we need to init the `Predef_models` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models class\n",
    "models = predef_models()\n",
    "\n",
    "# Select model\n",
    "img_shape = metadata.features['image'].shape[:-1] + (input_ids.shape[0],)\n",
    "model = models.select_model(model_name=p['model_name'], \n",
    "                            input_shape=img_shape,\n",
    "                            conv_reg=p['conv_reg'],\n",
    "                            dense_reg=p['dense_reg'],\n",
    "                            pre_training=p['pre_training']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the loss function and build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the loss function\n",
    "if p['loss'] == 'mse':\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "elif p['loss'] == 'huber':\n",
    "    loss = tf.keras.losses.Huber(delta=1.0)\n",
    "    \n",
    "elif p['loss'] == 'mean_absolute_error':\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "    \n",
    "msg = '{} loss function selected. Building the model...'.format(p['loss'])\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "metrics = ['mse', 'mean_absolute_error']\n",
    "model.compile(optimizer=Adam(learning_rate=p['learning_rate']),\n",
    "              loss=loss,\n",
    "              metrics=metrics\n",
    "             )\n",
    "msg = 'Model compiled!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the model architecture and number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates sys.stdout to the log file\n",
    "TeeLog = Tee_Logger(log_file_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish stdout duplication\n",
    "TeeLog.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set callback to save best model accordingly to the average of the Validation MAE of the last 30, 20 and 10 epochs. It also save the best model with out any average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sizes = [11, 21, 31]\n",
    "monitor='val_mean_absolute_error'\n",
    "\n",
    "save_best_model = save_best_model_base_on_CMA_Callback(monitor, avg_sizes)\n",
    "callbacks = [save_best_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tensorboard config (if active):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p['tensorboard']:\n",
    "    tb_dir_path = p['log_file_name'][:-4]+'_tensorboard'\n",
    "    try:\n",
    "        shutil.rmtree(tb_dir_path)\n",
    "    except OSError as e:\n",
    "        msg  = 'Tensorboard log dir {} could not be deleted!\\n\\nOSError: {}'.format(tb_dir_path, e)\n",
    "        logging.error(msg)\n",
    "        print(msg)\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir_path, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    \n",
    "    msg = 'Tensorboard file: {}'.format(tb_dir_path)\n",
    "    logging.info('\\n\\n'+msg+'\\n\\n')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Starting model training...'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LR_SCHEDULE given, then init lr scheduler callback\n",
    "# commented since Adam+decreasing the learning during training make the model more prompt to overfitting\n",
    "#if 'LR_SCHEDULE' in p.keys():\n",
    "#    finish_warmup_and_lr_schedule = lr_schedule_Callback(utils.lr_schedule, p['LR_SCHEDULE'])\n",
    "#    callbacks.append(finish_warmup_and_lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time before training\n",
    "tic = time.time()\n",
    "# Duplicates sys.stdout to the log file\n",
    "TeeLog = Tee_Logger(log_file_path)\n",
    "\n",
    "# Fit model\n",
    "n_train = metadata.splits['train'].num_examples\n",
    "history = model.fit(train_data,\n",
    "                    validation_data=val_data,\n",
    "                    epochs=p['number_of_epochs'],\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=p['verbose_level'],\n",
    "                    #steps_per_epoch=math.ceil(n_train/BATCH_SIZE),\n",
    "                    )\n",
    "toc = time.time()\n",
    "print('Training time (in mins): {}'.format((toc-tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish stdout duplication\n",
    "TeeLog.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_metrics(history=history.history, metrics=['loss']+metrics, p=p, figsize=(15,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Saiving trained model'\n",
    "logging.info(msg)\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(base_path, 'history.json'), 'w') as file:\n",
    "    json.dump(history.history, file, indent=4)\n",
    "    \n",
    "# Save CMA history\n",
    "# First wee need to convert from np.int64 and np.float64 to regular python int and float\n",
    "temp_dict = {}\n",
    "for key in save_best_model.CMA_history.keys():\n",
    "    temp_dict[key] = [[int(item[0]), float(item[1])] for item in save_best_model.CMA_history[key]]\n",
    "with open(os.path.join(base_path, 'CMA_history.json'), 'w') as file:\n",
    "    json.dump(temp_dict, file, indent=4)\n",
    "    \n",
    "# Save parameters\n",
    "with open(os.path.join(base_path, 'parameters.json'), 'w') as file:\n",
    "    json.dump(p, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history\n",
    "#path = ''\n",
    "#with open(os.path.join(path, 'history.json'), 'r') as file:\n",
    "#    history = json.load(file)\n",
    "# Save parameters\n",
    "#with open(os.path.join(base_path, 'parameters.json'), 'r') as file:\n",
    "#    p = json.load(file)\n",
    "#metrics = ['mse', 'mean_absolute_error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame to save model metrics\n",
    "metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, input_ids)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution plot\n",
    "model_eval.plot_error_dist(figsize=(20,7), hue='cell_cycle', sets=['train','val'])\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), x='cell_cycle', sets=['train','val'])\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), hue='cell_cycle')\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7,7), hue='cell_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics()\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Best model with no Center Moving Average (CMA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_0'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, input_ids)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "model_eval.plot_error_dist(figsize=(20,7), hue='cell_cycle', sets=['train','val'])\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), x='cell_cycle', sets=['train','val'])\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), hue='cell_cycle')\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7,7), hue='cell_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics()\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Best model wrt Central Moving Average of size 11 (CMA_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_11'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, input_ids)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "model_eval.plot_error_dist(figsize=(20,7), hue='cell_cycle', sets=['train','val'])\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), x='cell_cycle', sets=['train','val'])\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), hue='cell_cycle')\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7,7), hue='cell_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=11, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Best model wrt Central Moving Average of size 21 (CMA_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_21'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, input_ids)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "model_eval.plot_error_dist(figsize=(20,7), hue='cell_cycle', sets=['train','val'])\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), x='cell_cycle', sets=['train','val'])\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), hue='cell_cycle')\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7,7), hue='cell_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=21, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Best model wrt Central Moving Average of size 31 (CMA_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_31'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, input_ids)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "model_eval.plot_error_dist(figsize=(20,7), hue='cell_cycle', sets=['train','val'])\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), x='cell_cycle', sets=['train','val'])\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), hue='cell_cycle')\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7,7), hue='cell_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=31, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'test'\n",
    "metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'val'\n",
    "metrics_df[mask].sort_values(by=['MAE', 'Bias', 'Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "with open(os.path.join(base_path, 'metrics.csv'), 'w') as file:\n",
    "    metrics_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Notebook execution finished!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
