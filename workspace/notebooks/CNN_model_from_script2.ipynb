{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "The objective of this notebook is train and evaluate a given model specified in the parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "    \n",
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "p.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging configuration\n",
    "import logging\n",
    "log_file_path = p['log_file_name']\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='w', \n",
    "    level=getattr(logging, p['log_level'])\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing parameters with default values\n",
    "if not 'conv_reg' in p.keys():\n",
    "    p['conv_reg'] = [0,0]\n",
    "if not 'dense_reg' in p.keys():\n",
    "    p['dense_reg'] = [0,0]\n",
    "    \n",
    "if not 'verbose_level' in p.keys():\n",
    "    p['verbose_level'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Dataset:\\n\\t{}'.format(p['tf_ds_name'])\n",
    "msg += '\\n\\nData Augmentation:'\n",
    "msg += '\\n\\tRandom Flipping: {}\\n\\tRandom 90deg Rotations: {}'.format(p['random_horizontal_flipping'],p['random_90deg_rotations'])\n",
    "msg += '\\n\\tRandom centerd zoom: {}'.format(p['random_CenterZoom'])\n",
    "msg += '\\n\\nModel:'\n",
    "msg += '\\n\\tArchitecture: {}'.format(p['model_name'])\n",
    "msg += '\\n\\tConv layers regularization ([l1, l2]): {}'.format(p['conv_reg'])\n",
    "msg += '\\n\\tDense layers regularization ([l1, l2]): {}'.format(p['dense_reg'])\n",
    "msg += '\\n\\tLoss function: {}'.format(p['loss'])\n",
    "msg += '\\n\\tLearning rate: {}'.format(p['learning_rate'])\n",
    "msg += '\\n\\tEpochs: {}\\n\\n'.format(p['number_of_epochs'])\n",
    "logging.info(msg)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external libraries path\n",
    "EXTERNAL_LIBS_PATH = p['external_libs_path']\n",
    "if not os.path.exists(EXTERNAL_LIBS_PATH):\n",
    "    msg = 'External library path {} does not exist!'.format(EXTERNAL_LIBS_PATH)\n",
    "    logging.error(msg)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    msg='EXTERNAL_LIBS_PATH: {}'.format(EXTERNAL_LIBS_PATH)\n",
    "    print(msg)\n",
    "    logging.info(msg)\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, EXTERNAL_LIBS_PATH)\n",
    "# Load external libraries\n",
    "import Models as models_functions\n",
    "from Models import Predef_models as predef_models\n",
    "from Utils import Tee_Logger as Tee_Logger\n",
    "import Utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dirs where model output will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to avoid cleaning (deleting) model dir, then uncomment the next line:\n",
    "#p['clean_model_dir'] = 0\n",
    "\n",
    "base_path, model_path, checkpoints_path = utils.create_model_dirs(parameters=p)\n",
    "\n",
    "msg = 'Base path:\\n{}'.format(base_path)\n",
    "msg += '\\nModel path:\\n{}'.format(model_path)\n",
    "msg += '\\nCheckpoints path:\\n{}'.format(checkpoints_path)\n",
    "logging.info(msg)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf to ignore GPU\n",
    "if p['disable_gpu']:\n",
    "    msg = \"Cuda devices (GPUs) disabled\"\n",
    "    logging.info(msg)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "msg = 'Physical GPU devises:\\n{}'.format(physical_devices)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "#restrict GPU mem\n",
    "if p['set_memory_growth']:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        msg = 'GPU Memory limited!'\n",
    "    except:\n",
    "        msg = 'It was not possible to limit GPU memory'\n",
    "        \n",
    "    logging.info(msg)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessing parameters and information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data path\n",
    "pp_path = p['pp_path']\n",
    "\n",
    "with open(os.path.join(pp_path, 'params.json')) as pp_file:\n",
    "    pp_params = json.load(pp_file)\n",
    "msg = 'Loaded data preprocessing parameters from:\\n{}'.format(pp_file)\n",
    "logging.info(msg)\n",
    "seed = pp_params['seed']\n",
    "\n",
    "# Load Channels file\n",
    "with open(os.path.join(pp_path, 'channels.csv')) as channel_file:\n",
    "    channels = pd.read_csv(channel_file)\n",
    "msg = 'Loaded channels file from:\\n{}'.format(channel_file)\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_channels = p['input_channels']\n",
    "msg = 'Selected input channels:\\n{}'.format(selected_channels)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "# Get selected channel ids\n",
    "input_ids = np.array(channels.set_index(['name']).loc[selected_channels].channel_id.values)\n",
    "msg = 'Corresponding input channel ids:\\n{}'.format(input_ids)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where tf datasets are\n",
    "dataset, metadata = tfds.load(\n",
    "    name=p['tf_ds_name'], \n",
    "    data_dir=p['local_tf_datasets'], \n",
    "    # If False, returns a dictionary with all the features\n",
    "    as_supervised=True, \n",
    "    shuffle_files=p['shuffle_files'],\n",
    "    with_info=True)\n",
    "msg = 'Tensorflow dataset {} loaded from:\\n{}'.format(p['tf_ds_name'], p['local_tf_datasets'])\n",
    "logging.info(msg)\n",
    "\n",
    "# Load the splits\n",
    "train_data, val_data, test_data = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "Before training the network, we discriminate some channels, apply some linear transformations (90deg rotations and horizontal flipping) to augment the **Training** dataset, create the batches and shuffle them. Also, we perform other operations to improve performance.\n",
    "\n",
    "**Tune performance**<br>\n",
    "tf.data.Dataset.prefetch overlaps data preprocessing and model execution while training.\n",
    "It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "\n",
    "**Shuffling**<br>\n",
    "dataset.shuffle() Randomly shuffles the elements of this dataset.\n",
    "This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "**reshuffle_each_iteration** controls whether the shuffle order should be different for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "img_size = metadata.features['image'].shape[:-1]\n",
    "\n",
    "#@tf.function\n",
    "def zoom_image(tensor_img):\n",
    "    \n",
    "    def get_min_space(x):\n",
    "        t_x = tf.cast(tf.math.not_equal(tf.math.reduce_sum(x, axis=(1,2)), 0), \n",
    "                      dtype=tf.float32)\n",
    "        t_x_low = tf.math.argmax(t_x)\n",
    "        t_x_top = tf.math.argmax(tf.reverse(t_x, axis=[0]))\n",
    "        t_x_min = tf.math.minimum(t_x_low, t_x_top)\n",
    "        t_y = tf.cast(tf.math.not_equal(tf.math.reduce_sum(x, axis=(0,2)), 0), \n",
    "                      dtype=tf.float32)\n",
    "        t_y_low = tf.math.argmax(t_y)\n",
    "        t_y_top = tf.math.argmax(tf.reverse(t_y, axis=[0]))\n",
    "        t_y_min = tf.math.minimum(t_y_low, t_y_top)\n",
    "        \n",
    "        return tf.cast(tf.math.minimum(t_x_min, t_y_min), dtype=tf.float32)\n",
    "    \n",
    "    # get min space between the cell border and the image border\n",
    "    min_space = get_min_space(tensor_img)\n",
    "    \n",
    "    # Get the fraction of the origin image to crop without losing cell information\n",
    "    frac2crop = 1 - 2*(min_space/img_size[0])\n",
    "    #print('Cell fraction of image: ', frac2crop)\n",
    "    \n",
    "    # Get image with the cell at its maximum size\n",
    "    # Warning! function tf.image.central_crop requires frac2crop to be a fraction between 0 and 1!\n",
    "    # However, this is a tf graph function, which means that all calculations here returns a tensor.\n",
    "    # Therefore, you must change the origina library image_ops_impl.py for one from github which \n",
    "    # supports a tensor as input:\n",
    "    # wget https://raw.githubusercontent.com/tensorflow/tensorflow/b7a7f8d178254d1361d34dfc40a58b8dce48b9d7/tensorflow/python/ops/image_ops_impl.py\n",
    "    # https://github.com/tensorflow/tensorflow/pull/45613/files\n",
    "    tensor_img = tf.image.central_crop(tensor_img, frac2crop)\n",
    "    #print('Size after cropping: ', tensor_img.shape)\n",
    "    \n",
    "    # Select uniformly random the cell size (between 40% and 100% of the image)\n",
    "    cell_img_frac = tf.random.uniform(shape=[1], minval=0.4, maxval=1, dtype=tf.float32)\n",
    "    #print('Target cell img fraction: ', cell_img_frac)\n",
    "    \n",
    "    # Create temp image with cell size specified by cell_img_frac (random)\n",
    "    temp_size = tf.cast(cell_img_frac * img_size[0], dtype=tf.int32)\n",
    "    temp_size = tf.repeat(temp_size,2)\n",
    "    tensor_img = tf.image.resize(tensor_img,\n",
    "                                 size=temp_size,\n",
    "                                 method=tf.image.ResizeMethod.BILINEAR,\n",
    "                                 #method=tf.image.ResizeMethod.LANCZOS5,\n",
    "                                 preserve_aspect_ratio=False,\n",
    "                                 antialias=False)\n",
    "    #print(tensor_img.shape)\n",
    "    \n",
    "    # Resize image to original size\n",
    "    tensor_img = tf.image.resize_with_crop_or_pad(image=tensor_img,\n",
    "                                                  target_height=img_size[0],\n",
    "                                                  target_width=img_size[0])\n",
    "    \n",
    "    # Check if obtained fraction is close to target fraction\n",
    "    #min_space = get_min_space(tensor_img)\n",
    "    #frac2crop = tf.constant([1], dtype=tf.float32) - 2*(min_space/img_size[0])\n",
    "    #print('New cell img fraction: {}\\n'.format(frac2crop.numpy()))\n",
    "    \n",
    "    return tensor_img\n",
    "\n",
    "def filter_channels(image, target):\n",
    "    \"\"\"Function to discriminated undecired channels\"\"\"\n",
    "    \n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    \n",
    "    n_channels = metadata.features['image'].shape[-1]\n",
    "    n_selected_channels = input_ids.shape[-1]\n",
    "    \n",
    "    # Create projection matrix base on selected channels\n",
    "    projection_tensor = np.zeros((n_channels, n_selected_channels))\n",
    "    for col, row in enumerate(input_ids):\n",
    "        projection_tensor[row,col] = 1\n",
    "    projection_tensor = tf.constant(projection_tensor, dtype=tf.float32)\n",
    "    \n",
    "    new_shape = image.shape[:-1]+(n_selected_channels,)\n",
    "\n",
    "    return tf.reshape(tf.reshape(image, (-1,n_channels)) @ projection_tensor, (new_shape)), target\n",
    "\n",
    "def augment(image, target):\n",
    "    \"\"\"Function to augment dataset. After channel filtering, it flips \n",
    "    (horizontally) and rotates (0, 90, 180, 270 degrees) randomly the images.\n",
    "    \"\"\"\n",
    "        \n",
    "    image, target = filter_channels(image, target)\n",
    "    \n",
    "    # random Left and right flip\n",
    "    if p['random_horizontal_flipping']:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "    # random rotations\n",
    "    # Number of 90deg rotation\n",
    "    if p['random_90deg_rotations']:\n",
    "        k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k=k)\n",
    "    \n",
    "    # ZoomIn and ZoomOut\n",
    "    if p['random_CenterZoom']:\n",
    "        if len(image.shape) == 4:\n",
    "            image = tf.map_fn(zoom_image, image)\n",
    "        else:\n",
    "            image = zoom_image(image)\n",
    "    \n",
    "    return image, target\n",
    "\n",
    "def visualize_cell(image, target):\n",
    "    plt_size=np.array([5,4])\n",
    "    \n",
    "    plt.figure(figsize=plt_size)\n",
    "    plt.title('Original Cell')\n",
    "    plt.imshow(image.numpy()[:,:,10:13],\n",
    "               cmap=plt.cm.PiYG,\n",
    "               vmin=0, vmax=1)\n",
    "    plt.axis('equal')\n",
    "    plt.grid(False)\n",
    "    \n",
    "    if p['random_horizontal_flipping'] | p['random_90deg_rotations'] | p['random_CenterZoom']:\n",
    "        plt.figure(figsize=(4*plt_size[0],plt_size[1]))\n",
    "        for i in range(4):\n",
    "            img, _ = augment(image, target)\n",
    "            plt.subplot(1,4,i+1)\n",
    "            plt.title('Augmented Cell')\n",
    "            plt.imshow(img.numpy()[:,:,10:13],\n",
    "                       cmap=plt.cm.PiYG,\n",
    "                       vmin=0, vmax=1)\n",
    "            plt.axis('equal')\n",
    "            plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image from the training dataset\n",
    "image, target = next(iter(train_data))\n",
    "# Visualize the original vs. random flipping and rotations\n",
    "visualize_cell(image,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into one image and a random transformation (random rotation+random horizontal flippig):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets for training the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = p['BATCH_SIZE']\n",
    "buffer_size = 512\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = (\n",
    "    train_data\n",
    "    .shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_data = (\n",
    "    val_data\n",
    "    .map(filter_channels, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    test_data\n",
    "    .map(filter_channels, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Models are selected from a group of predefined models in the class `Predef_models` (in `Models.py`). The name of the selected model is specified in the parameter `p['model_method']`.\n",
    "\n",
    "First we need to init the `Predef_models` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models class\n",
    "models = predef_models()\n",
    "\n",
    "# Select model\n",
    "img_shape = metadata.features['image'].shape[:-1] + (input_ids.shape[0],)\n",
    "model = models.select_model(model_name=p['model_name'], \n",
    "                            input_shape=img_shape,\n",
    "                            conv_reg=p['conv_reg'],\n",
    "                            dense_reg=p['dense_reg']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the loss function and build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the loss function\n",
    "if p['loss'] == 'mse':\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "elif p['loss'] == 'huber':\n",
    "    loss = tf.keras.losses.Huber(delta=1.0)\n",
    "msg = '{} loss function selected. Building the model...'.format(p['loss'])\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "metrics = ['mse', 'mean_absolute_error']\n",
    "model.compile(optimizer=Adam(learning_rate=p['learning_rate']),\n",
    "              loss=loss,\n",
    "              metrics=metrics\n",
    "             )\n",
    "msg = 'Model compiled!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the model architecture and number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates sys.stdout to the log file\n",
    "TeeLog = Tee_Logger(log_file_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish stdout duplication\n",
    "TeeLog.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redirect the systems standard output to the logfile, so we can see the training process in the server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set callback to save best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath=checkpoints_path+'/checkpoint',\n",
    "#    save_weights_only=True,\n",
    "    filepath=checkpoints_path+'/best_model',\n",
    "    save_weights_only=False,\n",
    "    #monitor='val_loss',\n",
    "    monitor='val_mean_absolute_error',\n",
    "    mode='min',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True,\n",
    ")\n",
    "callbacks = [model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tensorboard config (if active):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p['tensorboard']:\n",
    "    tb_dir_path = p['log_file_name'][:-4]+'_tensorboard'\n",
    "    try:\n",
    "        shutil.rmtree(tb_dir_path)\n",
    "    except OSError as e:\n",
    "        msg  = 'Tensorboard log dir {} could not be deleted!\\n\\nOSError: {}'.format(tb_dir_path, e)\n",
    "        logging.error(msg)\n",
    "        print(msg)\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir_path, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    \n",
    "    msg = 'Tensorboard file: {}'.format(tb_dir_path)\n",
    "    logging.info('\\n\\n'+msg+'\\n\\n')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Starting model training...'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time before training\n",
    "tic = time.time()\n",
    "# Duplicates sys.stdout to the log file\n",
    "TeeLog = Tee_Logger(log_file_path)\n",
    "\n",
    "# Fit model\n",
    "n_train = metadata.splits['train'].num_examples\n",
    "history = model.fit(train_data,\n",
    "                    validation_data=val_data,\n",
    "                    epochs=p['number_of_epochs'],\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=p['verbose_level'],\n",
    "                    #steps_per_epoch=math.ceil(n_train/BATCH_SIZE),\n",
    "                    )\n",
    "toc = time.time()\n",
    "print('Training time (in mins): {}'.format((toc-tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish stdout duplication\n",
    "TeeLog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Saiving trained model'\n",
    "logging.info(msg)\n",
    "\n",
    "# Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(base_path, 'history.json'), 'w') as file:\n",
    "    json.dump(history.history, file, indent=4)\n",
    "    \n",
    "# Save parameters\n",
    "with open(os.path.join(base_path, 'parameters.json'), 'w') as file:\n",
    "    json.dump(p, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history\n",
    "#path = ''\n",
    "#with open(os.path.join(path, 'history.json'), 'r') as file:\n",
    "#    history = json.load(file)\n",
    "# Save parameters\n",
    "#with open(os.path.join(base_path, 'parameters.json'), 'r') as file:\n",
    "#    p = json.load(file)\n",
    "#metrics = ['mse', 'mean_absolute_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,18))\n",
    "models_functions.plot_loss(history.history, metrics, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First evaluation: Last model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_data, val_data, test_data)\n",
    "del(dataset, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, metadata = tfds.load(\n",
    "    name=p['tf_ds_name'], \n",
    "    data_dir=p['local_tf_datasets'], \n",
    "    # If False, returns a dictionary with all the features\n",
    "    as_supervised=False, \n",
    "    shuffle_files=False,\n",
    "    with_info=True)\n",
    "\n",
    "train_data, val_data, test_data = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = p['BATCH_SIZE']\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = (\n",
    "    train_data\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_data = (\n",
    "    val_data\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    test_data\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['y', 'y_hat', 'mapobject_id_cell', 'set']\n",
    "targets_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "dss = [train_data, val_data, test_data]\n",
    "ds_names = ['train', 'val', 'test']\n",
    "for ds, dsn in zip(dss, ds_names):\n",
    "    for cells in ds:\n",
    "        cell_ids = [cell_id.decode() for cell_id in cells['mapobject_id_cell'].numpy()]\n",
    "        cell_ids = np.asarray(cell_ids).reshape(-1,1)\n",
    "        cell_imgs, Y = filter_channels(cells['image'], cells['target'])\n",
    "        Y = Y.numpy()\n",
    "        Y_hat = model.predict(cell_imgs)\n",
    "        temp_df = pd.DataFrame(np.concatenate((Y, Y_hat), axis=1), columns=['y', 'y_hat'])\n",
    "        temp_df['mapobject_id_cell'] = cell_ids\n",
    "        temp_df['set'] = dsn\n",
    "        targets_df = pd.concat((targets_df, temp_df), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add perturbation info to the targets df\n",
    "with open(os.path.join(p['pp_path'], 'metadata.csv'), 'r') as file:\n",
    "    row_data_metadata = pd.read_csv(file)\n",
    "    row_data_metadata.mapobject_id_cell = row_data_metadata.mapobject_id_cell.astype(str)\n",
    "per_df = row_data_metadata[['mapobject_id_cell', 'perturbation']]\n",
    "targets_df = targets_df.merge(\n",
    "        per_df, \n",
    "        left_on='mapobject_id_cell',\n",
    "        right_on='mapobject_id_cell',\n",
    "        how='left',\n",
    ")\n",
    "targets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "Now lets see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_functions.plot_error_dist(df=targets_df, y_true='y', y_models=['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_functions.plot_y_dist(df=targets_df, y_true='y', y_hat='y_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "models_functions.plot_residuals(df=targets_df, y_true='y', y_hat='y_hat', hue='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs predicted\n",
    "models_functions.plot_y_vs_y_hat(df=targets_df, y_true='y', y_hat='y_hat', hue='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = models_functions.get_metrics(df=targets_df, y_true='y', y_hat='y_hat')\n",
    "metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second evaluation: Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "#model.load_weights(checkpoints_path+'/checkpoint')\n",
    "model.load_weights(os.path.join(checkpoints_path, 'best_model/variables/variables'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['y', 'y_hat', 'mapobject_id_cell', 'set']\n",
    "targets_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "dss = [train_data, val_data, test_data]\n",
    "ds_names = ['train', 'val', 'test']\n",
    "for ds, dsn in zip(dss, ds_names):\n",
    "    for cells in ds:\n",
    "        cell_ids = [cell_id.decode() for cell_id in cells['mapobject_id_cell'].numpy()]\n",
    "        cell_ids = np.asarray(cell_ids).reshape(-1,1)\n",
    "        cell_imgs, Y = filter_channels(cells['image'], cells['target'])\n",
    "        Y = Y.numpy()\n",
    "        Y_hat = model.predict(cell_imgs)\n",
    "        temp_df = pd.DataFrame(np.concatenate((Y, Y_hat), axis=1), columns=['y', 'y_hat'])\n",
    "        temp_df['mapobject_id_cell'] = cell_ids\n",
    "        temp_df['set'] = dsn\n",
    "        targets_df = pd.concat((targets_df, temp_df), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add perturbation info to the targets df\n",
    "with open(os.path.join(p['pp_path'], 'metadata.csv'), 'r') as file:\n",
    "    row_data_metadata = pd.read_csv(file)\n",
    "    row_data_metadata.mapobject_id_cell = row_data_metadata.mapobject_id_cell.astype(str)\n",
    "per_df = row_data_metadata[['mapobject_id_cell', 'perturbation']]\n",
    "targets_df = targets_df.merge(\n",
    "        per_df, \n",
    "        left_on='mapobject_id_cell',\n",
    "        right_on='mapobject_id_cell',\n",
    "        how='left',\n",
    ")\n",
    "targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sava targets info\n",
    "with open(os.path.join(base_path, 'targets.csv'), 'w') as file:\n",
    "    targets_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "Now lets see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_functions.plot_error_dist(df=targets_df, y_true='y', y_models=['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_functions.plot_y_dist(df=targets_df, y_true='y', y_hat='y_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "models_functions.plot_residuals(df=targets_df, y_true='y', y_hat='y_hat', hue='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs predicted\n",
    "models_functions.plot_y_vs_y_hat(df=targets_df, y_true='y', y_hat='y_hat', hue='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = models_functions.get_metrics(df=targets_df, y_true='y', y_hat='y_hat')\n",
    "\n",
    "# Sava metrics info\n",
    "with open(os.path.join(base_path, 'metrics.csv'), 'w') as file:\n",
    "    metric_values.to_csv(file, index=False)\n",
    "metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Notebook execution finished!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
