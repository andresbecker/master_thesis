{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "The objective of this notebook is train and evaluate a given model specified in the parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import socket\n",
    "\n",
    "# Set terminal output (to send mesages to the terminal stdout)\n",
    "terminal_output = open('/dev/stdout', 'w')\n",
    "print('Execution of Notebook started at {}'.format(datetime.now()), file=terminal_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external libraries\n",
    "key = 'external_libs_path'\n",
    "if socket.gethostname() == 'hughes-machine':\n",
    "    external_libs_path = '/home/hhughes/Documents/Master_Thesis/Project/workspace/libs'\n",
    "else:\n",
    "    external_libs_path= '/storage/groups/ml01/code/andres.becker/master_thesis/workspace/libs'\n",
    "print('External libs path: \\n'+external_libs_path, file=terminal_output)\n",
    "\n",
    "if not os.path.exists(external_libs_path):\n",
    "    msg = 'External library path {} does not exist!'.format(external_libs_path)\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, external_libs_path)\n",
    "# Load external libraries\n",
    "from Models_V2 import Predef_models as predef_models\n",
    "from Utils import Tee_Logger as Tee_Logger\n",
    "#from Utils import lr_schedule_Callback\n",
    "#from Utils import save_best_model_Callback\n",
    "from Utils import save_best_model_base_on_CMA_Callback\n",
    "from Utils import evaluate_model\n",
    "import Utils as utils\n",
    "import Data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "#PARAMETERS_FILE = '/home/hhughes/Documents/Master_Thesis/Project/workspace/scripts/Parameters/model_params/local/Quick_test_local.json'\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "\n",
    "# Open parameters\n",
    "if os.path.exists(PARAMETERS_FILE):\n",
    "    with open(PARAMETERS_FILE) as file:\n",
    "        p = json.load(file)\n",
    "else:\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "\n",
    "# IMPORTANT\n",
    "# All outputs are saved using the model name and the name of the parameters file\n",
    "# For instance, if model='ResNet50V2' and param file='test_1.json', then\n",
    "# the model will be saved saved at p['model_path']/ResNet50V2/test_1\n",
    "\n",
    "# Save parameter file path\n",
    "p['parameters_file_path'] = PARAMETERS_FILE\n",
    "p['external_libs_path'] = external_libs_path\n",
    "# check and set default parameters\n",
    "p, info = utils.set_model_default_parameters(p)\n",
    "print(info)\n",
    "print(p.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=p['log_file'],\n",
    "    filemode='w', \n",
    "    level=getattr(logging, 'INFO')\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))\n",
    "msg = 'Log file: '+p['log_file']\n",
    "print(msg)\n",
    "logging.info(info)\n",
    "# Print location of the log file into the terminal\n",
    "print(msg, file=terminal_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dirs where model output will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to avoid cleaning (deleting) model dir, then uncomment the next line:\n",
    "#p['clean_model_dir'] = 0\n",
    "\n",
    "base_path, model_path, checkpoints_path = utils.create_model_dirs(parameters=p)\n",
    "\n",
    "msg = 'Base path:\\n{}'.format(base_path)\n",
    "msg += '\\nModel path:\\n{}'.format(model_path)\n",
    "msg += '\\nCheckpoints path:\\n{}'.format(checkpoints_path)\n",
    "logging.info(msg)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf to ignore GPU\n",
    "if p['disable_gpu']:\n",
    "    msg = \"Cuda devices (GPUs) disabled\"\n",
    "    logging.info(msg)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "msg = 'Physical GPU devises:\\n{}'.format(physical_devices)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "#restrict GPU mem\n",
    "if p['set_memory_growth']:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        msg = 'GPU Memory limited!'\n",
    "    except:\n",
    "        msg = 'It was not possible to limit GPU memory'\n",
    "        \n",
    "    logging.info(msg)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Channels file\n",
    "# TODO: CHANGE THIS (and parameters) to specify only the channel_df path, since it is the\n",
    "# only stuff that is being used\n",
    "with open(os.path.join(p['pp_path'], 'channels.csv')) as file:\n",
    "    channels = pd.read_csv(file)\n",
    "msg = 'Loaded channels file from:\\n{}'.format(file)\n",
    "logging.info(msg)\n",
    "\n",
    "selected_channels = p['input_channels']\n",
    "msg = 'Selected input channels:\\n{}'.format(selected_channels)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "# Get selected channel ids\n",
    "input_ids = np.array(channels.set_index(['name']).loc[selected_channels].channel_id.values)\n",
    "msg = 'Corresponding input channel ids:\\n{}'.format(input_ids)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where tf datasets are\n",
    "dataset, metadata = tfds.load(\n",
    "    name=p['tf_ds_name'], \n",
    "    data_dir=p['local_tf_datasets'], \n",
    "    # If False, returns a dictionary with all the features\n",
    "    as_supervised=True, \n",
    "    shuffle_files=p['shuffle_files'],\n",
    "    with_info=True)\n",
    "msg = 'Tensorflow dataset {} loaded from:\\n{}'.format(p['tf_ds_name'], p['local_tf_datasets'])\n",
    "logging.info(msg)\n",
    "\n",
    "# Load the splits\n",
    "train_data, val_data = dataset['train'], dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "Before training the network, we discriminate some channels, apply some linear transformations (90deg rotations and horizontal flipping) to augment the **Training** dataset, create the batches and shuffle them. Also, we perform other operations to improve performance.\n",
    "\n",
    "**Tune performance**<br>\n",
    "tf.data.Dataset.prefetch overlaps data preprocessing and model execution while training.\n",
    "It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "\n",
    "**Shuffling**<br>\n",
    "dataset.shuffle() Randomly shuffles the elements of this dataset.\n",
    "This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "**reshuffle_each_iteration** controls whether the shuffle order should be different for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets for training the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some necessary stuff for data preprocessing\n",
    "input_shape = np.array(metadata.features['image'].shape)\n",
    "projection_tensor = Data_augmentation.get_projection_tensor(input_shape, input_ids)\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "train_data, val_data = Data_augmentation.prepare_train_and_val_TFDS_2(train_data, \n",
    "                                                                      val_data,\n",
    "                                                                      projection_tensor,\n",
    "                                                                      p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into one image and a random transformation (random rotation+random horizontal flippig):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch to print a cell\n",
    "images, targets = next(iter(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image from the training batch\n",
    "img_idx = np.random.randint(0, p['BATCH_SIZE'])\n",
    "# add an extra dim (like a batch)\n",
    "image = tf.expand_dims(images[img_idx], axis=0)\n",
    "Data_augmentation.visualize_data_augmentation(image, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Models are selected from a group of predefined models in the class `Predef_models` (in `Models.py`). The name of the selected model is specified in the parameter `p['model_method']`.\n",
    "\n",
    "First we need to init the `Predef_models` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models class\n",
    "models = predef_models()\n",
    "\n",
    "# Select model\n",
    "img_shape = metadata.features['image'].shape[:-1] + (input_ids.shape[0],)\n",
    "model = models.select_model(model_name=p['model_name'], \n",
    "                            input_shape=img_shape,\n",
    "                            conv_reg=p['conv_reg'],\n",
    "                            dense_reg=p['dense_reg'],\n",
    "                            bias_l2_reg=p['bias_l2_reg'],\n",
    "                            pre_training=p['pre_training']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the loss function and build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: print model losses (one for each layer regularized (1 for bias reg and 1 for kernel reg))\n",
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the loss function\n",
    "if p['loss'] == 'mse':\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "elif p['loss'] == 'huber':\n",
    "    loss = tf.keras.losses.Huber(delta=1.0)\n",
    "    \n",
    "elif p['loss'] == 'mean_absolute_error':\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "    \n",
    "msg = '{} loss function selected. Building the model...'.format(p['loss'])\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "metrics = ['mse', 'mean_absolute_error']\n",
    "model.compile(optimizer=Adam(learning_rate=p['learning_rate']),\n",
    "              loss=loss,\n",
    "              metrics=metrics\n",
    "             )\n",
    "msg = 'Model compiled!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the model architecture and number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates sys.stdout to the log file\n",
    "model.summary(print_fn=utils.print_stdout_and_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set callback to save best model accordingly to the average of the Validation MAE of the last 30, 20 and 10 epochs. It also save the best model with out any average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sizes = [11, 21, 31]\n",
    "monitor='val_mean_absolute_error'\n",
    "\n",
    "save_best_model = save_best_model_base_on_CMA_Callback(monitor, avg_sizes)\n",
    "callbacks = [save_best_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tensorboard config (if active):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p['tensorboard']:\n",
    "    tb_dir_path = p['log_file'][:-4]+'_tensorboard'\n",
    "    try:\n",
    "        shutil.rmtree(tb_dir_path)\n",
    "    except OSError as e:\n",
    "        msg  = 'Tensorboard log dir {} could not be deleted!\\n\\nOSError: {}'.format(tb_dir_path, e)\n",
    "        logging.error(msg)\n",
    "        print(msg)\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir_path, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    \n",
    "    msg = 'Tensorboard file: {}'.format(tb_dir_path)\n",
    "    logging.info('\\n\\n'+msg+'\\n\\n')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Starting model training...'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LR_SCHEDULE given, then init lr scheduler callback\n",
    "# commented since Adam+decreasing the learning during training make the model more prompt to overfitting\n",
    "#if 'LR_SCHEDULE' in p.keys():\n",
    "#    finish_warmup_and_lr_schedule = lr_schedule_Callback(utils.lr_schedule, p['LR_SCHEDULE'])\n",
    "#    callbacks.append(finish_warmup_and_lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time before training\n",
    "tic = time.time()\n",
    "# Duplicates sys.stdout to the log file\n",
    "TeeLog = Tee_Logger(p['log_file'])\n",
    "\n",
    "# Fit model\n",
    "n_train = metadata.splits['train'].num_examples\n",
    "history = model.fit(train_data,\n",
    "                    validation_data=val_data,\n",
    "                    epochs=p['number_of_epochs'],\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=p['verbose_level'],\n",
    "                    #steps_per_epoch=math.ceil(n_train/BATCH_SIZE),\n",
    "                    )\n",
    "toc = time.time()\n",
    "print('Training time (in mins): {}'.format((toc-tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish stdout duplication\n",
    "TeeLog.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_metrics(history=history.history, metrics=['loss']+metrics, p=p, figsize=(15,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Saiving trained model'\n",
    "logging.info(msg)\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(base_path, 'history.json'), 'w') as file:\n",
    "    json.dump(history.history, file, indent=4)\n",
    "    \n",
    "# Save CMA history\n",
    "# First wee need to convert from np.int64 and np.float64 to regular python int and float\n",
    "temp_dict = {}\n",
    "for key in save_best_model.CMA_history.keys():\n",
    "    temp_dict[key] = [[int(item[0]), float(item[1])] for item in save_best_model.CMA_history[key]]\n",
    "with open(os.path.join(base_path, 'CMA_history.json'), 'w') as file:\n",
    "    json.dump(temp_dict, file, indent=4)\n",
    "    \n",
    "# Save parameters\n",
    "with open(os.path.join(base_path, 'parameters.json'), 'w') as file:\n",
    "    json.dump(p, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history\n",
    "#path = ''\n",
    "#with open(os.path.join(path, 'history.json'), 'r') as file:\n",
    "#    history = json.load(file)\n",
    "# Save parameters\n",
    "#with open(os.path.join(base_path, 'parameters.json'), 'r') as file:\n",
    "#    p = json.load(file)\n",
    "#metrics = ['mse', 'mean_absolute_error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame to save model metrics\n",
    "metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, projection_tensor)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics()\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Best model with no Center Moving Average (CMA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_0'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, projection_tensor)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=0, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Best model wrt Central Moving Average of size 11 (CMA_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_11'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, projection_tensor)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=11, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Best model wrt Central Moving Average of size 21 (CMA_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_21'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, projection_tensor)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=21, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Best model wrt Central Moving Average of size 31 (CMA_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_31'\n",
    "# Load weights\n",
    "model.set_weights(save_best_model.best_models[eval_name][3])\n",
    "# Save model\n",
    "model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, model, projection_tensor)\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history.history, \n",
    "                         CMA_history=save_best_model.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=31, \n",
    "                       CMA=save_best_model.best_models[eval_name][1], \n",
    "                       CMA_Std=save_best_model.best_models[eval_name][2], \n",
    "                       Epoch=save_best_model.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'test'\n",
    "metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'val'\n",
    "metrics_df[mask].sort_values(by=['MAE', 'Bias', 'Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "with open(os.path.join(base_path, 'metrics.csv'), 'w') as file:\n",
    "    metrics_df.to_csv(file, index=False)\n",
    "\n",
    "# Save a copy into a common dir to comparation among models\n",
    "temp_path = os.path.join(p['model_path'], 'Model_Metrics')\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "with open(os.path.join(temp_path, p['basename']+'.csv'), 'w') as file:\n",
    "    metrics_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Notebook execution finished!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
