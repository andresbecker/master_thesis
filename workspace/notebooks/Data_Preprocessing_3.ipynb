{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /home/hhughes/Documents/Master_Thesis/Project\n",
      "DATA_DIR: /home/hhughes/Documents/Master_Thesis/Project/datasets/raw\n",
      "\n",
      "Available local wells: \n",
      " ['I11', 'I09', 'J10']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = os.path.realpath(os.path.join(os.path.abspath(''),'../..'))\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    print('ERROR!, base path {} does not exist! Setting to None'.format(BASE_DIR))\n",
    "    BASE_DIR = None\n",
    "else:\n",
    "    print('BASE_DIR: {}'.format(BASE_DIR))\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'datasets', 'raw')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print('ERROR!, data path {} does not exist! Setting to None'.format(DATA_DIR))\n",
    "    DATA_DIR = None\n",
    "else:\n",
    "    print('DATA_DIR: {}\\n'.format(DATA_DIR))\n",
    "    \n",
    "# Add BASE_DIR to sys paths (for loading libraries)\n",
    "sys.path.insert(1, os.path.join(BASE_DIR, 'workspace'))\n",
    "# Load mpp_data library to convert raw data into images\n",
    "from pelkmans.mpp_data import MPPData as MPPData\n",
    "\n",
    "# List available local Wells\n",
    "wells = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "print('Available local wells: \\n', wells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load raw data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you only want to load some specific wells, rename 'wells'\n",
    "wells = ['J10']\n",
    "\n",
    "data_params = {\n",
    "    # where to read data from\n",
    "    'data_dirs': [os.path.join(DATA_DIR, well) for well in wells],\n",
    "    'dir_type': 'hannah',\n",
    "    # make results reproducible\n",
    "    'seed': 42,\n",
    "    # input/output definition\n",
    "    'input_channels': [\n",
    "        '00_DAPI',\n",
    "        '07_H2B',\n",
    "        '01_CDK9_pT186',\n",
    "        '03_CDK9',\n",
    "        '05_GTF2B',\n",
    "        '07_SETD1A',\n",
    "        '08_H3K4me3',\n",
    "        '09_SRRM2',\n",
    "        '10_H3K27ac',\n",
    "        '11_KPNA2_MAX',\n",
    "        '12_RB1_pS807_S811',\n",
    "        '13_PABPN1',\n",
    "        '14_PCNA',\n",
    "        '15_SON',\n",
    "        '16_H3',\n",
    "        '17_HDAC3',\n",
    "        '19_KPNA1_MAX',\n",
    "        '20_SP100',\n",
    "        '21_NCL',\n",
    "        '01_PABPC1',\n",
    "        '02_CDK7',\n",
    "        '03_RPS6',\n",
    "        '05_Sm',\n",
    "        '07_POLR2A',\n",
    "        '09_CCNT1',\n",
    "        '10_POL2RA_pS2',\n",
    "        '11_PML',\n",
    "        '12_YAP1',\n",
    "        '13_POL2RA_pS5',\n",
    "        '15_U2SNRNPB',\n",
    "        '18_NONO',\n",
    "        '20_ALYREF',\n",
    "        '21_COIL',\n",
    "    ],\n",
    "    'output_channels': ['00_EU'],\n",
    "    'aggregate_output': 'avg', # None results in output images, 'max', 'avg' aggregate output channels and output a single number\n",
    "    # train/val/test split\n",
    "    'train_frac': 0.5,\n",
    "    'val_frac': 0.4,\n",
    "    'img_size': 224,\n",
    "    # normalisation\n",
    "    'background_value': os.path.join(DATA_DIR, 'secondary_only_relative_normalisation.csv'),\n",
    "    'normalise': True,\n",
    "    'percentile': 98.0,\n",
    "    # Condition\n",
    "    'cell_cycle_file': os.path.join(DATA_DIR, 'cell_cycle_classification.csv'),\n",
    "    #'condition': ['G1', 'S', 'G2'],\n",
    "    'condition': ['cell_cycle'],\n",
    "    'subset_to_cell_cycle': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "missing background value for channels ['00_EU', '09_SRRM2_ILASTIK', '15_SON_ILASTIK']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"if p.get('condition', None) is not None:\\n    train.add_conditions(p['condition'], \\n                         cell_cycle_file=p.get('cell_cycle_file', None), \\n                         subset=p.get('subset_to_cell_cycle', False))\\n    val.add_conditions(p['condition'], \\n                       cell_cycle_file=p.get('cell_cycle_file', None), \\n                       subset=p.get('subset_to_cell_cycle', False))\\n    test.add_conditions(p['condition'], \\n                        cell_cycle_file=p.get('cell_cycle_file', None), \\n                        subset=p.get('subset_to_cell_cycle', False))\\n\\n# subset to channels\\n        if p.get('channels', None) is not None:\\n            train.subset_channels(p['channels'])\\n            val.subset_channels(p['channels'])\\n            test.subset_channels(p['channels'])\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = data_params\n",
    "\n",
    "mpp_datas = {'train': [], 'val': [], 'test': []}\n",
    "for data_dir in p['data_dirs']:\n",
    "    # Load data as an MPPData object\n",
    "    mpp_data = MPPData.from_data_dir(data_dir,\n",
    "                                     dir_type=p['dir_type'],\n",
    "                                     seed=p['seed'])\n",
    "\n",
    "    # Subtract background  values for each channel\n",
    "    if p['normalise']:\n",
    "        mpp_data.subtract_background(p['background_value'])\n",
    "    \n",
    "    # Split data into train, validation and test\n",
    "    train, val, test = mpp_data.train_val_test_split(p['train_frac'], p['val_frac'])\n",
    "    \n",
    "    # Save well data in dic\n",
    "    mpp_datas['train'].append(train)\n",
    "    mpp_datas['test'].append(test)\n",
    "    mpp_datas['val'].append(val)\n",
    "    \n",
    "    # To save memory\n",
    "    del(train)\n",
    "    del(val)\n",
    "    del(test)\n",
    "    del(mpp_data)\n",
    "    \n",
    "# merge data from all the loaded wells\n",
    "train = MPPData.concat(mpp_datas['train'])\n",
    "val = MPPData.concat(mpp_datas['val'])\n",
    "test = MPPData.concat(mpp_datas['test'])\n",
    "del(mpp_datas)\n",
    "\n",
    "# Normalize train, val and test using the normalization parameters\n",
    "# got from the train data (inner percentile% of train data)\n",
    "if p['normalise']:\n",
    "    rescale_values = train.rescale_intensities_per_channel(percentile=p['percentile'])\n",
    "    _ = val.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    _ = test.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    p['normalise_rescale_values'] = list(rescale_values)\n",
    "\n",
    "# Filter cells depending on its condition (cell state (G1, S or G2))\n",
    "'''if p.get('condition', None) is not None:\n",
    "    train.add_conditions(p['condition'], \n",
    "                         cell_cycle_file=p.get('cell_cycle_file', None), \n",
    "                         subset=p.get('subset_to_cell_cycle', False))\n",
    "    val.add_conditions(p['condition'], \n",
    "                       cell_cycle_file=p.get('cell_cycle_file', None), \n",
    "                       subset=p.get('subset_to_cell_cycle', False))\n",
    "    test.add_conditions(p['condition'], \n",
    "                        cell_cycle_file=p.get('cell_cycle_file', None), \n",
    "                        subset=p.get('subset_to_cell_cycle', False))\n",
    "\n",
    "# subset to channels\n",
    "        if p.get('channels', None) is not None:\n",
    "            train.subset_channels(p['channels'])\n",
    "            val.subset_channels(p['channels'])\n",
    "            test.subset_channels(p['channels'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images\n",
    "train_dataset = np.array(train.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "val_dataset = np.array(val.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "test_dataset = np.array(test.get_object_imgs(data='MPP', img_size=p['img_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channels ids (proteins) which will be used to predict transcripcion rate\n",
    "input_ids = list(train.channels.set_index('name').loc[p['input_channels']]['channel_id'])\n",
    "# Get id of the channel that measure trancripcion rate\n",
    "output_ids = list(train.channels.set_index('name').loc[p['output_channels']]['channel_id'])\n",
    "\n",
    "# Create regressor (y)\n",
    "if p['aggregate_output'] == 'avg':\n",
    "    train_dataset_y = np.array([img[img!=0].mean() for img in train_dataset[:,:,:,output_ids]])\n",
    "    val_dataset_y = np.array([img[img!=0].mean() for img in val_dataset[:,:,:,output_ids]])\n",
    "    test_dataset_y = np.array([img[img!=0].mean() for img in test_dataset[:,:,:,output_ids]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "dataset_name = '184A1_hannah_EU_regression'\n",
    "outdir = os.path.join(BASE_DIR, 'datasets', dataset_name)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "np.savez(os.path.join(outdir, 'train_dataset.npz'), x=train_dataset[:,:,:,input_ids], y=train_dataset_y)\n",
    "np.savez(os.path.join(outdir, 'val_dataset.npz'), x=val_dataset[:,:,:,input_ids], y=val_dataset_y)\n",
    "np.savez(os.path.join(outdir, 'test_dataset.npz'), x=test_dataset[:,:,:,input_ids], y=test_dataset_y)\n",
    "\n",
    "# save params\n",
    "json.dump(data_params, open(os.path.join(outdir, 'params.json'), 'w'), indent=4)\n",
    "\n",
    "# save metadata\n",
    "train.metadata.to_csv(os.path.join(outdir, 'train_metadata.csv'))\n",
    "val.metadata.to_csv(os.path.join(outdir, 'val_metadata.csv'))\n",
    "test.metadata.to_csv(os.path.join(outdir, 'test_metadata.csv'))\n",
    "pd.concat([train.metadata, val.metadata, test.metadata]).to_csv(os.path.join(outdir, 'metadata.csv'))\n",
    "train.channels.to_csv(os.path.join(outdir, 'channels.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
