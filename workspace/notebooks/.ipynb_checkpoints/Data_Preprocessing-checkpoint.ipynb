{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ment to convert raw cell data from several wells into multichannel images (along with its corresponding metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = os.path.realpath(os.path.join(os.path.abspath(''),'../..'))\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    print('ERROR!, base path {} does not exist! Setting to None'.format(BASE_DIR))\n",
    "    BASE_DIR = None\n",
    "else:\n",
    "    print('BASE_DIR: {}'.format(BASE_DIR))\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'datasets', 'raw')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print('ERROR!, data path {} does not exist! Setting to None'.format(DATA_DIR))\n",
    "    DATA_DIR = None\n",
    "else:\n",
    "    print('DATA_DIR: {}\\n'.format(DATA_DIR))\n",
    "    \n",
    "# Add BASE_DIR to sys paths (for loading libraries)\n",
    "sys.path.insert(1, os.path.join(BASE_DIR, 'workspace'))\n",
    "# Load mpp_data library to convert raw data into images\n",
    "from pelkmans.mpp_data import MPPData as MPPData\n",
    "\n",
    "# List available local Wells\n",
    "wells = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "print('Available local wells: \\n', wells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you only want to load some specific wells, rename 'wells'\n",
    "wells = ['I09']\n",
    "\n",
    "data_params = {\n",
    "    # where to read data from\n",
    "    'data_dirs': [os.path.join(DATA_DIR, well) for well in wells],\n",
    "    'dir_type': 'hannah',\n",
    "    # make results reproducible\n",
    "    'seed': 42,\n",
    "    # input/output definition\n",
    "    'input_channels': [\n",
    "        '00_DAPI',\n",
    "        '07_H2B',\n",
    "        '01_CDK9_pT186',\n",
    "        '03_CDK9',\n",
    "        '05_GTF2B',\n",
    "        '07_SETD1A',\n",
    "        '08_H3K4me3',\n",
    "        '09_SRRM2',\n",
    "        '10_H3K27ac',\n",
    "        '11_KPNA2_MAX',\n",
    "        '12_RB1_pS807_S811',\n",
    "        '13_PABPN1',\n",
    "        '14_PCNA',\n",
    "        '15_SON',\n",
    "        '16_H3',\n",
    "        '17_HDAC3',\n",
    "        '19_KPNA1_MAX',\n",
    "        '20_SP100',\n",
    "        '21_NCL',\n",
    "        '01_PABPC1',\n",
    "        '02_CDK7',\n",
    "        '03_RPS6',\n",
    "        '05_Sm',\n",
    "        '07_POLR2A',\n",
    "        '09_CCNT1',\n",
    "        '10_POL2RA_pS2',\n",
    "        '11_PML',\n",
    "        '12_YAP1',\n",
    "        '13_POL2RA_pS5',\n",
    "        '15_U2SNRNPB',\n",
    "        '18_NONO',\n",
    "        '20_ALYREF',\n",
    "        '21_COIL',\n",
    "    ],\n",
    "    'output_channels': ['00_EU'],\n",
    "    'aggregate_output': 'avg', # None results in output images, 'max', 'avg' aggregate output channels and output a single number\n",
    "    # train/val/test split\n",
    "    'train_frac': 0.8,\n",
    "    'val_frac': 0.1,\n",
    "    'img_size': 224,\n",
    "    # normalisation\n",
    "    'background_value': os.path.join(DATA_DIR, 'secondary_only_relative_normalisation.csv'),\n",
    "    'normalise': True,\n",
    "    'percentile': 98.0,\n",
    "    # Add Cell cycle to metadata\n",
    "    'add_cell_cycle_to_metadata': True,\n",
    "    'cell_cycle_file': os.path.join(DATA_DIR, 'cell_cycle_classification.csv'),\n",
    "    # Add well info to metadata (cell_type, perturbation and duration)\n",
    "    'add_well_info_to_metadata': True,\n",
    "    'well_info_file': os.path.join(DATA_DIR, 'wells_metadata.csv'),\n",
    "    # Fitering\n",
    "    'filter_criteria': ['is_border_cell', 'is_mitotic'],\n",
    "    'filter_values': [0, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = data_params\n",
    "\n",
    "mpp_datas = {'train': [], 'val': [], 'test': []}\n",
    "for data_dir in p['data_dirs']:\n",
    "    # Load data as an MPPData object\n",
    "    mpp_data = MPPData.from_data_dir(data_dir,\n",
    "                                     dir_type=p['dir_type'],\n",
    "                                     seed=p['seed'])\n",
    "    \n",
    "    # Remove unwanted cells\n",
    "    if p.get('filter_criteria', None) is not None:\n",
    "        mpp_data.filter_cells(p['filter_criteria'], p['filter_values'])\n",
    "\n",
    "    # Subtract background  values for each channel\n",
    "    if p['normalise']:\n",
    "        mpp_data.subtract_background(p['background_value'])\n",
    "        \n",
    "    \n",
    "    # Split data into train, validation and test\n",
    "    train, val, test = mpp_data.train_val_test_split(p['train_frac'], p['val_frac'])\n",
    "    \n",
    "    # Save well data in dic\n",
    "    mpp_datas['train'].append(train)\n",
    "    mpp_datas['test'].append(test)\n",
    "    mpp_datas['val'].append(val)\n",
    "    \n",
    "    # Release memory\n",
    "    del(train)\n",
    "    del(val)\n",
    "    del(test)\n",
    "    del(mpp_data)\n",
    "    \n",
    "# merge data from all the loaded wells\n",
    "train = MPPData.concat(mpp_datas['train'])\n",
    "val = MPPData.concat(mpp_datas['val'])\n",
    "test = MPPData.concat(mpp_datas['test'])\n",
    "del(mpp_datas)\n",
    "\n",
    "# Normalize train, val and test using the normalization parameters\n",
    "# got from the train data (inner percentile% of train data)\n",
    "if p['normalise']:\n",
    "    rescale_values = train.rescale_intensities_per_channel(percentile=p['percentile'])\n",
    "    _ = val.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    _ = test.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    p['normalise_rescale_values'] = list(rescale_values)\n",
    "\n",
    "# Add cell cycle to metadata (G1, S, G2)\n",
    "if p['add_cell_cycle_to_metadata']:\n",
    "    train.add_cell_cycle_to_metadata(p['cell_cycle_file'])\n",
    "    val.add_cell_cycle_to_metadata(p['cell_cycle_file'])\n",
    "    test.add_cell_cycle_to_metadata(p['cell_cycle_file'])\n",
    "\n",
    "if p['add_well_info_to_metadata']:\n",
    "    train.add_well_info_to_metadata(p['well_info_file'])\n",
    "    val.add_well_info_to_metadata(p['well_info_file'])\n",
    "    val.add_well_info_to_metadata(p['well_info_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "dataset_name = '184A1_hannah_EU_regression'\n",
    "outdir = os.path.join(BASE_DIR, 'datasets', dataset_name)\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save params\n",
    "json.dump(data_params, open(os.path.join(outdir, 'params.json'), 'w'), indent=4)\n",
    "\n",
    "# save metadata\n",
    "train.metadata.to_csv(os.path.join(outdir, 'train_metadata.csv'))\n",
    "val.metadata.to_csv(os.path.join(outdir, 'val_metadata.csv'))\n",
    "test.metadata.to_csv(os.path.join(outdir, 'test_metadata.csv'))\n",
    "pd.concat([train.metadata, val.metadata, test.metadata]).to_csv(os.path.join(outdir, 'metadata.csv'))\n",
    "train.channels.to_csv(os.path.join(outdir, 'channels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channels ids (proteins) which will be used to predict transcripcion rate\n",
    "input_ids = list(train.channels.set_index('name').loc[p['input_channels']]['channel_id'])\n",
    "# Get id of the channel that measure trancripcion rate\n",
    "output_ids = list(train.channels.set_index('name').loc[p['output_channels']]['channel_id'])\n",
    "\n",
    "# get images\n",
    "train_dataset = np.array(train.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(train)\n",
    "val_dataset = np.array(val.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(val)\n",
    "test_dataset = np.array(test.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(test)\n",
    "\n",
    "# Create responce variable (y)\n",
    "if p['aggregate_output'] == 'avg':\n",
    "    train_dataset_y = np.array([img[img!=0].mean() for img in train_dataset[:,:,:,output_ids]])\n",
    "    val_dataset_y = np.array([img[img!=0].mean() for img in val_dataset[:,:,:,output_ids]])\n",
    "    test_dataset_y = np.array([img[img!=0].mean() for img in test_dataset[:,:,:,output_ids]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "np.savez(os.path.join(outdir, 'train_dataset.npz'), x=train_dataset[:,:,:,input_ids], y=train_dataset_y)\n",
    "del(train_dataset)\n",
    "np.savez(os.path.join(outdir, 'val_dataset.npz'), x=val_dataset[:,:,:,input_ids], y=val_dataset_y)\n",
    "del(val_dataset)\n",
    "np.savez(os.path.join(outdir, 'test_dataset.npz'), x=test_dataset[:,:,:,input_ids], y=test_dataset_y)\n",
    "del(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
