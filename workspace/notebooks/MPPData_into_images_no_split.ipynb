{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing to Transforming MPP data into images and predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ment to convert raw cell data from several wells into multichannel images (along with its corresponding mask, targets and metadata).\n",
    "\n",
    "Data was taken from:\n",
    "`/storage/groups/ml01/datasets/raw/20201020_Pelkmans_NascentRNA_hannah.spitzer/` and server `vicb-submit-01`. \n",
    "\n",
    "In the preprocessing done in this notebook. The objective of this preprocessing is to create a 'imaged' version of the MPP data.\n",
    "\n",
    "The discretization of the channels (input_channels) and the selection of the target variable is done during the convertion into tensorflow dataset!\n",
    "\n",
    "Considerations:\n",
    "- NO discrimination of channels is done! All the channels are saved in the same order and all of them are also projected into a scalars and saved as target. However, if input_channels and output_channels are given in the parameters file, then the filtering of channels/targets is done during saving into disk.\n",
    "- To avoid data duplication, the cell images for each well are saved right after the preprocessing and not at the end.\n",
    "- NO train, val and test splitting is done here! That (and data normalization) is done during the creation of the TFDS.\n",
    "- There are several ways of saving the images. This behaviour is defined by the parameter `img_saving_mode`:\n",
    "    - **original_img**: save original cell image with its original size and shape (it could be recangular or squared). The drawback of this method is that images will be saved with different sizes and size ratios.\n",
    "    - **original_img_and_squared**: save original cell image without fixed size but fixed shape (squared). Despite that the all images are saved with the same shape (squared), the drawback of this method is that images will be saved with different sizes.\n",
    "    - **original_img_and_fixed_size**: save original cell image with fixed size and shape (squared). The drawback of this method is that if zoomin wants to be used as a data augmentation technique, then each image will need to be processed individually on the fly to avoid cropping the cell information.\n",
    "    - **fixed_cell_size**: save image with a fixed size and shape (squared)maximizing the cell size within the image. This mean that up or down sampling may be needed. The drawback of this method is that the some distortion of the original data (image) is inevitable during up or down samplig (zoom in/out). There are several option for the up/down sampling (interpolation) which can be selected through the parameter `img_interpolation_method`. To see the complete list of available interpolation methods, please visit:<br>https://www.tensorflow.org/api_docs/python/tf/image/ResizeMethod\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import socket\n",
    "\n",
    "# Set terminal output (to send mesages to the terminal stdout)\n",
    "terminal_output = open('/dev/stdout', 'w')\n",
    "print('Execution of Notebook started at {}'.format(datetime.now()), file=terminal_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load external libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external libraries\n",
    "if socket.gethostname() == 'hughes-machine':\n",
    "    external_libs_path = '/home/hhughes/Documents/Master_Thesis/Project/workspace/libs'\n",
    "else:\n",
    "    external_libs_path= '/storage/groups/ml01/code/andres.becker/master_thesis/workspace/libs'\n",
    "print('External libs path: \\n'+external_libs_path, file=terminal_output)\n",
    "\n",
    "if not os.path.exists(external_libs_path):\n",
    "    msg = 'External library path {} does not exist!'.format(external_libs_path)\n",
    "    raise Exception(msg)\n",
    "    \n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, external_libs_path)\n",
    "# Load external libraries\n",
    "from pelkmans.mpp_data_V2 import MPPData as MPPData\n",
    "from Utils import create_directory as create_directory\n",
    "from Utils import print_stdout_and_log as print_stdout_and_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "#PARAMETERS_FILE = '/home/hhughes/Documents/Master_Thesis/Project/workspace/scripts/Parameters/MPP_to_imgs_no_split_local.json'\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "    \n",
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "    \n",
    "# Save parameter file path and libs path\n",
    "p['parameters_file_path'] = PARAMETERS_FILE\n",
    "p['external_libs_path'] = external_libs_path\n",
    "\n",
    "# Set some default parameters in case they are not given\n",
    "if 'input_channels' not in p.keys():\n",
    "    p['input_channels'] = None\n",
    "    \n",
    "if 'output_channels' not in p.keys():\n",
    "    p['output_channels'] = None\n",
    "\n",
    "for key in p.keys():\n",
    "    print_stdout_and_log('{}: {}'.format(key, p[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging configuration\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename=p['log_file'],\n",
    "    filemode='w', \n",
    "    level=getattr(logging, 'INFO')\n",
    ")\n",
    "print_stdout_and_log('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths and Load external libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data path\n",
    "DATA_DIR = p['raw_data_dir']\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise Exception('Data path {} does not exist!'.format(DATA_DIR))\n",
    "print_stdout_and_log('DATA_DIR: {}'.format(DATA_DIR))\n",
    "\n",
    "# Create dirs to save data\n",
    "outdir = p['output_pp_data_path']\n",
    "create_directory(dir_path=outdir, clean_if_exist=False)\n",
    "\n",
    "# Create directories to save images\n",
    "output_data_path = os.path.join(outdir, p['output_pp_data_dir_name'])\n",
    "create_directory(dir_path=output_data_path, clean_if_exist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Prepare selected data to process (wells and I/O channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available data (Perturbations and Wells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stdout_and_log('Reading local available perturbations-wells...')\n",
    "# Save available local Perturbations and Wells\n",
    "perturbations = [per for per in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, per))]\n",
    "local_data = {}\n",
    "#print('Local available perturbations-wells:\\n')\n",
    "for per in perturbations:\n",
    "    pertur_dir = os.path.join(DATA_DIR, per)\n",
    "    wells = [w for w in os.listdir(pertur_dir) if os.path.isdir(os.path.join(pertur_dir, w))]\n",
    "    #print('{}\\n\\t{}\\n'.format(p, wells))\n",
    "    local_data[per] = wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Perturbations and its wells to process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Local available perturbations-wells:\\n{}'.format(local_data)\n",
    "print(msg)\n",
    "logging.debug(msg)\n",
    "\n",
    "# In case you only want to load some specific perturbations and/or wells here:\n",
    "#selected_data = {\n",
    "#    '184A1_hannah_unperturbed': ['I11', 'I09'],\n",
    "#    '184A1_hannah_TSA': ['J20', 'I16'],\n",
    "#}\n",
    "\n",
    "# Load perturbations-wells from parameters file\n",
    "selected_data = p['perturbations_and_wells']\n",
    "# How many wlls will be processed?\n",
    "n_wells = 0\n",
    "for key in list(selected_data.keys()):\n",
    "    n_wells += len(selected_data[key])\n",
    "\n",
    "print('\\nSelected perturbations-wells:\\n{}'.format(selected_data))\n",
    "\n",
    "#Generate and save data dirs\n",
    "data_dirs = []\n",
    "for per in selected_data.keys():\n",
    "    for w in selected_data[per]:\n",
    "        d = os.path.join(DATA_DIR, per, w)\n",
    "        data_dirs.append(d)\n",
    "        if not os.path.exists(d):\n",
    "            msg = '{} does not exist!\\nCheck if selected_data contain elements only from local_data dict.'.format(d)\n",
    "            raise Exception(msg)\n",
    "p['data_dirs'] = data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Process data and save into disk as images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Starting processing of {} wells...'.format(n_wells)\n",
    "logging.info(msg)\n",
    "\n",
    "metadata_df = pd.DataFrame()\n",
    "channels_df = pd.DataFrame()\n",
    "\n",
    "for w, data_dir in enumerate(p['data_dirs'], 1):\n",
    "    msg = 'Processing well {}/{} from dir {}...'.format(w, n_wells, data_dir)\n",
    "    logging.info(msg)\n",
    "    print('\\n\\n'+msg)\n",
    "    # Load data as an MPPData object\n",
    "    mpp_temp = MPPData.from_data_dir(data_dir, dir_type=p['dir_type'])\n",
    "    \n",
    "    # Validate same channels across wells\n",
    "    if channels_df.shape[0] == 0:\n",
    "        channels_df = mpp_temp.channels\n",
    "    if not all(channels_df.name == mpp_temp.channels.name):\n",
    "        raise Exception('Channels across MPPData instances are not the same!')\n",
    "    \n",
    "    # Add cell cycle to metadata (G1, S, G2)\n",
    "    # Important! If mapobject_id_cell is not in cell_cycle_file =>\n",
    "    # its corresponding cell is in Mitosis phase!\n",
    "    if p['add_cell_cycle_to_metadata']:\n",
    "        print_stdout_and_log('Adding cell cycle to metadata...')\n",
    "        mpp_temp.add_cell_cycle_to_metadata(os.path.join(DATA_DIR, p['cell_cycle_file']))\n",
    "    \n",
    "    # Add well info to metadata\n",
    "    if p['add_well_info_to_metadata']:\n",
    "        print_stdout_and_log('Adding well info to metadata...')\n",
    "        mpp_temp.add_well_info_to_metadata(os.path.join(DATA_DIR, p['well_info_file']))\n",
    "    \n",
    "    # Remove unwanted cells\n",
    "    if p.get('filter_criteria', None) is not None:\n",
    "        print_stdout_and_log('Removing unwanted cells...')\n",
    "        mpp_temp.filter_cells(p['filter_criteria'], p['filter_values'])\n",
    "\n",
    "    # Subtract background values for each channel\n",
    "    if p['subtract_background']:\n",
    "        print_stdout_and_log('Subtracting background...')\n",
    "        mpp_temp.subtract_background(os.path.join(DATA_DIR, p['background_value']))\n",
    "    \n",
    "    # Project every uni-channel images into a scalar for further analysis\n",
    "    if p['project_into_scalar']:\n",
    "        print_stdout_and_log('Projecting data...')\n",
    "        mpp_temp.add_scalar_projection(p['aggregate_output'])\n",
    "        \n",
    "        \n",
    "    # Convert MPP into image and save to disk\n",
    "    print_stdout_and_log('Creating well images and saving into disk...')\n",
    "    mpp_temp.save_img_mask_and_target_into_fs(outdir=output_data_path,\n",
    "                                              input_channels=p['input_channels'], \n",
    "                                              output_channels=p['output_channels'],\n",
    "                                              projection_method=p['aggregate_output'],\n",
    "                                              img_size=p['img_size'],\n",
    "                                              img_saving_mode=p['img_saving_mode'],\n",
    "                                              img_interpolation_method=p['img_interpolation_method'],\n",
    "                                              pad=0, \n",
    "                                              dtype=p['images_dtype']\n",
    "                                             )\n",
    "\n",
    "    # Concatenate well metadata\n",
    "    if metadata_df.shape[0] == 0:\n",
    "        metadata_df = mpp_temp.metadata\n",
    "        channels_df = mpp_temp.channels\n",
    "    else:\n",
    "        metadata_df = pd.concat((metadata_df, mpp_temp.metadata), axis=0, ignore_index=True)\n",
    "    \n",
    "    del(mpp_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.- Save Metadata and parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Saving Parameters and Metadata...'\n",
    "logging.info(msg)\n",
    "\n",
    "# save params\n",
    "with open(os.path.join(outdir, 'params.json'), 'w') as file:\n",
    "    json.dump(p, file, indent=4)\n",
    "\n",
    "# save metadata\n",
    "with open(os.path.join(outdir, 'metadata.csv'), 'w') as file:\n",
    "    metadata_df.to_csv(file, index=False)\n",
    "\n",
    "# Save used channels\n",
    "with open(os.path.join(outdir, 'channels.csv'), 'w') as file:\n",
    "    channels_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load one saved file and take a look into the content to see if everithing was done correctlly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = np.random.choice(metadata_df['mapobject_id_cell'].values)\n",
    "file = os.path.join(output_data_path, str(cell_id)+'.npz')\n",
    "cell = np.load(file)\n",
    "cell_img = cell['img']\n",
    "cell_img = cell_img / np.max(cell_img, axis=(0,1))\n",
    "cell_mask = cell['mask']\n",
    "cell_targets = cell['targets']\n",
    "\n",
    "print('Cell image shape: {}\\n'.format(cell_img.shape))\n",
    "print('Cell mask shape: {}\\n'.format(cell_mask.shape))\n",
    "print('Cell target shape: {}\\n'.format(cell_targets.shape))\n",
    "\n",
    "# Now take a look into its image\n",
    "plt.figure(figsize=(2 * 10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cell_img[:,:,10:13],\n",
    "           cmap=plt.cm.PiYG,\n",
    "           vmin=0, vmax=1,\n",
    "           aspect='equal'\n",
    "          )\n",
    "plt.title('Cell image')\n",
    "plt.grid(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cell_mask,\n",
    "           cmap=plt.cm.Greys,\n",
    "           vmin=0, vmax=1,\n",
    "           aspect='equal'\n",
    "          )\n",
    "plt.title('Cell mask')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print('\\nCell targets: {}\\n'.format(cell_targets))\n",
    "\n",
    "logging.info('\\n\\nPREPROCESSING FINISHED!!!!----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
