{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ment to convert raw cell data from several wells into multichannel images (along with its corresponding metadata).\n",
    "\n",
    "Data was taken from:\n",
    "`/storage/groups/ml01/datasets/raw/20201020_Pelkmans_NascentRNA_hannah.spitzer/` and server `vicb-submit-01`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and set Directories paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = os.path.realpath(os.path.join(os.path.abspath(''),'../..'))\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    raise Exception('Base path {} does not exist!'.format(BASE_DIR))\n",
    "else:\n",
    "    print('BASE_DIR: {}'.format(BASE_DIR))\n",
    "    \n",
    "# Add BASE_DIR to sys paths (for loading libraries)\n",
    "sys.path.insert(1, os.path.join(BASE_DIR, 'workspace'))\n",
    "# Load mpp_data library to convert raw data into images\n",
    "from pelkmans.mpp_data import MPPData as MPPData\n",
    "    \n",
    "PARAMETERS_FILE = os.path.join(BASE_DIR, 'workspace/scripts/Parameters/temp_parameters.json')\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "#del(p['_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set raw data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = p['raw_data_dir']\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise Exception('Data path {} does not exist!'.format(DATA_DIR))\n",
    "else:\n",
    "    print('DATA_DIR: {}'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available data (Perturbations and Wells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save available local Perturbations and Wells\n",
    "perturbations = [per for per in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, per))]\n",
    "local_data = {}\n",
    "#print('Local available perturbations-wells:\\n')\n",
    "for per in perturbations:\n",
    "    pertur_dir = os.path.join(DATA_DIR, per)\n",
    "    wells = [w for w in os.listdir(pertur_dir) if os.path.isdir(os.path.join(pertur_dir, w))]\n",
    "    #print('{}\\n\\t{}\\n'.format(p, wells))\n",
    "    local_data[per] = wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Perturbations and its wells to process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Local available perturbations-wells:\\n{}'.format(local_data))\n",
    "\n",
    "# In case you only want to load some specific perturbations and/or wells:\n",
    "selected_data = {\n",
    "    '184A1_hannah_unperturbed': ['I11', 'I09'],\n",
    "    '184A1_hannah_TSA': ['J20', 'I16'],\n",
    "}\n",
    "\n",
    "selected_data = p['perturbations_and_wells']\n",
    "\n",
    "# Process all available data:\n",
    "#selected_data = local_data\n",
    "\n",
    "print('\\nSelected perturbations-wells:\\n{}'.format(selected_data))\n",
    "\n",
    "#Generate and save data dirs\n",
    "data_dirs = []\n",
    "for per in selected_data.keys():\n",
    "    for w in selected_data[per]:\n",
    "        d = os.path.join(DATA_DIR, per, w)\n",
    "        data_dirs.append(d)\n",
    "        if not os.path.exists(d):\n",
    "            raise Exception('{} does not exist!\\nCheck if selected_data contain elements only from local_data dict.'.format(d))\n",
    "p['data_dirs'] = data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_dir in p['data_dirs']:\n",
    "    print('\\nProcessing dir {}...'.format(data_dir))\n",
    "    # Load data as an MPPData object\n",
    "    mpp_temp = MPPData.from_data_dir(data_dir,\n",
    "                                     dir_type=p['dir_type'],\n",
    "                                     seed=p['seed'])\n",
    "    \n",
    "    # Add cell cycle to metadata (G1, S, G2)\n",
    "    # Important! If mapobject_id_cell is not in cell_cycle_file =>\n",
    "    # its corresponding cell is in Mitosis phase!\n",
    "    if p['add_cell_cycle_to_metadata']:\n",
    "        print('Adding cell cycle to metadata...')\n",
    "        mpp_temp.add_cell_cycle_to_metadata(os.path.join(DATA_DIR, p['cell_cycle_file']))\n",
    "    \n",
    "    # Add well info to metadata\n",
    "    if p['add_well_info_to_metadata']:\n",
    "        print('Adding well info to metadata...')\n",
    "        mpp_temp.add_well_info_to_metadata(os.path.join(DATA_DIR, p['well_info_file']))\n",
    "    \n",
    "    # Remove unwanted cells\n",
    "    if p.get('filter_criteria', None) is not None:\n",
    "        print('Removing unwanted cells...')\n",
    "        mpp_temp.filter_cells(p['filter_criteria'], p['filter_values'])\n",
    "\n",
    "    # Subtract background  values for each channel\n",
    "    if p['subtract_background']:\n",
    "        print('Subtracting background...')\n",
    "        mpp_temp.subtract_background(os.path.join(DATA_DIR, p['background_value']))\n",
    "    \n",
    "    # Project every uni-channel images into a scalar for further analysis\n",
    "    if p['project_into_scalar']:\n",
    "        print('\\nProjecting data...')\n",
    "        mpp_temp.add_scalar_projection(p['method'])\n",
    "        \n",
    "    # Split data into train, validation and test\n",
    "    train_temp, val_temp, test_temp = mpp_temp.train_val_test_split(p['train_frac'], p['val_frac'])\n",
    "    del(mpp_temp)\n",
    "    \n",
    "    if p['convert_into_image']:\n",
    "        print('Converting data into images...')\n",
    "        train_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        val_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        test_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "    \n",
    "    # Concatenate wells\n",
    "    # Check first if data sets are already defined\n",
    "    if 'train' not in globals().keys():\n",
    "        train, val, test = train_temp, val_temp, test_temp\n",
    "    else:\n",
    "        print('Merging data with loaded well...')\n",
    "        val.merge_instances([val_temp])\n",
    "        del(val_temp)\n",
    "        test.merge_instances([test_temp])\n",
    "        del(test_temp)\n",
    "        train.merge_instances([train_temp])\n",
    "        del(train_temp)\n",
    "\n",
    "# Normalize train, val and test using the normalization parameters\n",
    "# got from the train data (inner percentile% of train data)\n",
    "if p['normalise']:\n",
    "    print('\\nNormalizing data...')\n",
    "    rescale_values = train.rescale_intensities_per_channel(percentile=p['percentile'], )\n",
    "    _ = val.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    _ = test.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    p['normalise_rescale_values'] = list(rescale_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "Prepare to save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# create dir\n",
    "outdir = p['output_data_dir']\n",
    "if os.path.exists(outdir):\n",
    "    print('Warning! Directory {} already exist! Deleting...\\n'.format(outdir))\n",
    "    try:\n",
    "        shutil.rmtree(outdir)\n",
    "    except OSError as e:\n",
    "        print('Dir {} could not be deleted!\\n\\nOSError: {}'.format(outdir, e))\n",
    "\n",
    "print('Creating dir: {}'.format(outdir))\n",
    "os.makedirs(outdir, exist_ok=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channels ids (proteins) which will be used to predict transcripcion rate\n",
    "input_ids = list(train.channels.set_index('name').loc[p['input_channels']]['channel_id'])\n",
    "# Get id of the channel that measure trancripcion rate\n",
    "output_ids = list(train.channels.set_index('name').loc[p['output_channels']]['channel_id'])\n",
    "# add output channel id after the input channels ids\n",
    "channels_ids = input_ids + output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save metadata and used parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save params\n",
    "json.dump(p, open(os.path.join(outdir, 'params.json'), 'w'), indent=4)\n",
    "\n",
    "# save metadata\n",
    "train.metadata.to_csv(os.path.join(outdir, 'train_metadata.csv'))\n",
    "val.metadata.to_csv(os.path.join(outdir, 'val_metadata.csv'))\n",
    "test.metadata.to_csv(os.path.join(outdir, 'test_metadata.csv'))\n",
    "pd.concat([train.metadata, val.metadata, test.metadata]).to_csv(os.path.join(outdir, 'metadata.csv'))\n",
    "\n",
    "# Save used channels\n",
    "#train.channels.to_csv(os.path.join(outdir, 'channels.csv'))\n",
    "train.channels.set_index('channel_id').loc[channels_ids].to_csv(os.path.join(outdir, 'channels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note! instead of calculating the response value (y) here and save\n",
    "# it separatelly, instead we will do it on the modeling part\n",
    "\n",
    "\"\"\"\n",
    "# get images\n",
    "train_dataset = np.array(train.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(train)\n",
    "val_dataset = np.array(val.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(val)\n",
    "test_dataset = np.array(test.get_object_imgs(data='MPP', img_size=p['img_size']))\n",
    "del(test)\n",
    "\n",
    "# Create responce variable (y)\n",
    "if p['aggregate_output'] == 'avg':\n",
    "    train_dataset_y = np.array([img[img!=0].mean() for img in train_dataset[:,:,:,output_ids]])\n",
    "    val_dataset_y = np.array([img[img!=0].mean() for img in val_dataset[:,:,:,output_ids]])\n",
    "    test_dataset_y = np.array([img[img!=0].mean() for img in test_dataset[:,:,:,output_ids]])\n",
    "\n",
    "# Save datasets\n",
    "np.savez(os.path.join(outdir, 'train_dataset.npz'), x=train_dataset[:,:,:,input_ids], y=train_dataset_y)\n",
    "del(train_dataset)\n",
    "np.savez(os.path.join(outdir, 'val_dataset.npz'), x=val_dataset[:,:,:,input_ids], y=val_dataset_y)\n",
    "del(val_dataset)\n",
    "np.savez(os.path.join(outdir, 'test_dataset.npz'), x=test_dataset[:,:,:,input_ids], y=test_dataset_y)\n",
    "del(test_dataset)\n",
    "\"\"\"\n",
    "\n",
    "if p['convert_into_image']:\n",
    "    # get images and mask, save them and delete vars\n",
    "    print('Saving train images and masks...')\n",
    "    np.save(os.path.join(outdir, 'train_images.npy'), train.images[:,:,:,channels_ids])\n",
    "    del(train.images)\n",
    "    np.save(os.path.join(outdir, 'train_mask.npy'), train.masks)\n",
    "    del(train.masks)\n",
    "    del(train)\n",
    "\n",
    "    print('Saving validation images and masks...')\n",
    "    np.save(os.path.join(outdir, 'val_images.npy'), val.images[:,:,:,channels_ids])\n",
    "    del(val.images)\n",
    "    np.save(os.path.join(outdir, 'val_mask.npy'), val.masks)\n",
    "    del(val.masks)\n",
    "    del(val)\n",
    "\n",
    "    print('Saving test images and masks...')\n",
    "    np.save(os.path.join(outdir, 'test_images.npy'), test.images[:,:,:,channels_ids])\n",
    "    del(test.images)\n",
    "    np.save(os.path.join(outdir, 'test_mask.npy'), test.masks)\n",
    "    del(test.masks)\n",
    "    del(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
