{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ment to convert raw cell data from several wells into multichannel images (along with its corresponding metadata).\n",
    "\n",
    "Data was taken from:\n",
    "`/storage/groups/ml01/datasets/raw/20201020_Pelkmans_NascentRNA_hannah.spitzer/` and server `vicb-submit-01`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:17.394504Z",
     "iopub.status.busy": "2020-11-20T09:42:17.392384Z",
     "iopub.status.idle": "2020-11-20T09:42:17.760711Z",
     "shell.execute_reply": "2020-11-20T09:42:17.760180Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:17.764253Z",
     "iopub.status.busy": "2020-11-20T09:42:17.763823Z",
     "iopub.status.idle": "2020-11-20T09:42:17.769299Z",
     "shell.execute_reply": "2020-11-20T09:42:17.768701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "    \n",
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "p.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the loaded parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:17.774068Z",
     "iopub.status.busy": "2020-11-20T09:42:17.773646Z",
     "iopub.status.idle": "2020-11-20T09:42:17.776793Z",
     "shell.execute_reply": "2020-11-20T09:42:17.776121Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths and Load external libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:17.781480Z",
     "iopub.status.busy": "2020-11-20T09:42:17.780994Z",
     "iopub.status.idle": "2020-11-20T09:42:19.149565Z",
     "shell.execute_reply": "2020-11-20T09:42:19.148851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data path\n",
    "DATA_DIR = p['raw_data_dir']\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise Exception('Data path {} does not exist!'.format(DATA_DIR))\n",
    "else:\n",
    "    print('DATA_DIR: {}'.format(DATA_DIR))\n",
    "\n",
    "# Load external libraries path\n",
    "EXTERNAL_LIBS_PATH = p['external_libs_path']\n",
    "if not os.path.exists(EXTERNAL_LIBS_PATH):\n",
    "    raise Exception('External library path {} does not exist!'.format(EXTERNAL_LIBS_PATH))\n",
    "else:\n",
    "    print('EXTERNAL_LIBS_PATH: {}'.format(EXTERNAL_LIBS_PATH))\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, EXTERNAL_LIBS_PATH)\n",
    "# Load external libraries\n",
    "from pelkmans.mpp_data import MPPData as MPPData\n",
    "from pelkmans.mpp_data import normalize_and_save_MPPData_images as normalize_and_save\n",
    "from pelkmans.mpp_data import get_image_normalization_vals as get_normalization_vals\n",
    "from pelkmans.mpp_data import get_concatenated_metadata as get_concatenated_metadata\n",
    "\n",
    "# Set logging configuration\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename=p['log_file'],\n",
    "    filemode='w', \n",
    "    level=getattr(logging, p['log_level'])\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available data (Perturbations and Wells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:19.154745Z",
     "iopub.status.busy": "2020-11-20T09:42:19.154293Z",
     "iopub.status.idle": "2020-11-20T09:42:19.156903Z",
     "shell.execute_reply": "2020-11-20T09:42:19.156476Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Reading local available perturbations-wells...')\n",
    "# Save available local Perturbations and Wells\n",
    "perturbations = [per for per in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, per))]\n",
    "local_data = {}\n",
    "#print('Local available perturbations-wells:\\n')\n",
    "for per in perturbations:\n",
    "    pertur_dir = os.path.join(DATA_DIR, per)\n",
    "    wells = [w for w in os.listdir(pertur_dir) if os.path.isdir(os.path.join(pertur_dir, w))]\n",
    "    #print('{}\\n\\t{}\\n'.format(p, wells))\n",
    "    local_data[per] = wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Perturbations and its wells to process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:19.160770Z",
     "iopub.status.busy": "2020-11-20T09:42:19.160389Z",
     "iopub.status.idle": "2020-11-20T09:42:19.162644Z",
     "shell.execute_reply": "2020-11-20T09:42:19.162201Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Local available perturbations-wells:\\n{}'.format(local_data)\n",
    "print(msg)\n",
    "logging.debug(msg)\n",
    "\n",
    "# In case you only want to load some specific perturbations and/or wells here:\n",
    "#selected_data = {\n",
    "#    '184A1_hannah_unperturbed': ['I11', 'I09'],\n",
    "#    '184A1_hannah_TSA': ['J20', 'I16'],\n",
    "#}\n",
    "\n",
    "# Load perturbations-wells from parameters file\n",
    "selected_data = p['perturbations_and_wells']\n",
    "\n",
    "print('\\nSelected perturbations-wells:\\n{}'.format(selected_data))\n",
    "\n",
    "#Generate and save data dirs\n",
    "data_dirs = []\n",
    "for per in selected_data.keys():\n",
    "    for w in selected_data[per]:\n",
    "        d = os.path.join(DATA_DIR, per, w)\n",
    "        data_dirs.append(d)\n",
    "        if not os.path.exists(d):\n",
    "            msg = '{} does not exist!\\nCheck if selected_data contain elements only from local_data dict.'.format(d)\n",
    "            logging.error(msg)\n",
    "            raise Exception(msg)\n",
    "p['data_dirs'] = data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:19.170970Z",
     "iopub.status.busy": "2020-11-20T09:42:19.164472Z",
     "iopub.status.idle": "2020-11-20T09:42:57.130597Z",
     "shell.execute_reply": "2020-11-20T09:42:57.129415Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Starting data loading process...')\n",
    "\n",
    "data = {\n",
    "    'train':[],\n",
    "    'val':[],\n",
    "    'test':[]\n",
    "}\n",
    "\n",
    "for data_dir in p['data_dirs']:\n",
    "    msg = 'Processing dir {}...'.format(data_dir)\n",
    "    logging.info(msg)\n",
    "    print('\\n\\n'+msg)\n",
    "    # Load data as an MPPData object\n",
    "    mpp_temp = MPPData.from_data_dir(data_dir,\n",
    "                                     dir_type=p['dir_type'],\n",
    "                                     seed=p['seed'])\n",
    "    \n",
    "    # Add cell cycle to metadata (G1, S, G2)\n",
    "    # Important! If mapobject_id_cell is not in cell_cycle_file =>\n",
    "    # its corresponding cell is in Mitosis phase!\n",
    "    if p['add_cell_cycle_to_metadata']:\n",
    "        msg = 'Adding cell cycle to metadata...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.add_cell_cycle_to_metadata(os.path.join(DATA_DIR, p['cell_cycle_file']))\n",
    "    \n",
    "    # Add well info to metadata\n",
    "    if p['add_well_info_to_metadata']:\n",
    "        msg = 'Adding well info to metadata...'\n",
    "        logging.info('Adding well info to metadata...')\n",
    "        print(msg)\n",
    "        mpp_temp.add_well_info_to_metadata(os.path.join(DATA_DIR, p['well_info_file']))\n",
    "    \n",
    "    # Remove unwanted cells\n",
    "    if p.get('filter_criteria', None) is not None:\n",
    "        msg = 'Removing unwanted cells...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.filter_cells(p['filter_criteria'], p['filter_values'])\n",
    "\n",
    "    # Subtract background  values for each channel\n",
    "    if p['subtract_background']:\n",
    "        print('Subtracting background...')\n",
    "        mpp_temp.subtract_background(os.path.join(DATA_DIR, p['background_value']))\n",
    "    \n",
    "    # Project every uni-channel images into a scalar for further analysis\n",
    "    if p['project_into_scalar']:\n",
    "        msg = 'Projecting data...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.add_scalar_projection(p['method'])\n",
    "        \n",
    "    # Split data into train, validation and test\n",
    "    msg = 'Spliting data into train, validation and test'\n",
    "    logging.info(msg)\n",
    "    print(msg)\n",
    "    train_temp, val_temp, test_temp = mpp_temp.train_val_test_split(p['train_frac'], p['val_frac'])\n",
    "    del(mpp_temp)\n",
    "    \n",
    "    if p['convert_into_image']:\n",
    "        msg = 'Converting data into images...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        train_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Train dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        val_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Validation dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        test_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Test dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "    \n",
    "    # Validate same channels across wells\n",
    "    if len(data['val']) > 0:\n",
    "        if not all(data['val'][0].channels.name == val_temp.channels.name):\n",
    "            raise Exception('Channels across MPPData instances are not the same!')\n",
    "            \n",
    "    data['val'].append(val_temp)\n",
    "    del(val_temp)\n",
    "    data['test'].append(test_temp)\n",
    "    del(test_temp)\n",
    "    data['train'].append(train_temp)\n",
    "    del(train_temp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:42:57.137588Z",
     "iopub.status.busy": "2020-11-20T09:42:57.136548Z",
     "iopub.status.idle": "2020-11-20T09:43:07.639696Z",
     "shell.execute_reply": "2020-11-20T09:43:07.639158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize train, val and test using the normalization parameters\n",
    "# got from the train data (inner percentile% of train data)\n",
    "if p['normalise']:\n",
    "    msg = 'Normalizing data...'\n",
    "    logging.info(msg)\n",
    "    if p['convert_into_image']:\n",
    "        # Onle get the normalization parameters here and normalize \n",
    "        # images during saving into files\n",
    "        rescale_values = get_normalization_vals(\n",
    "            instance_dict=data['train'], \n",
    "            percentile=p['percentile'])\n",
    "    if not p['remove_original_data']:\n",
    "        # Get normalization values and normalize original MPPData\n",
    "        pass\n",
    "        # TODO: Rewrite rescale_intensities_per_channel to deal with\n",
    "        # a dictionary of MPPData instances, instead of a big numpy array\n",
    "        # (this is to avoid the duplication in memory)\n",
    "        #rescale_values = train.rescale_intensities_per_channel(percentile=p['percentile'], )\n",
    "        #_ = val.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "        #_ = test.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    p['normalise_rescale_values'] = list(rescale_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:07.644869Z",
     "iopub.status.busy": "2020-11-20T09:43:07.644421Z",
     "iopub.status.idle": "2020-11-20T09:43:07.672071Z",
     "shell.execute_reply": "2020-11-20T09:43:07.672369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge metadata and Normalize projected values\n",
    "msg = 'Normalizing Projected data...'\n",
    "logging.info(msg)\n",
    "    \n",
    "metadata, rescale_values = get_concatenated_metadata(\n",
    "    mppdata_dict=data,\n",
    "    normalize=(p['normalise'] & p['project_into_scalar']),\n",
    "    norm_key='train',\n",
    "    projection_method=p['method'],\n",
    "    percentile=p['percentile'])\n",
    "\n",
    "p['normalise_rescale_values_scalars'] = list(rescale_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:07.698714Z",
     "iopub.status.busy": "2020-11-20T09:43:07.697938Z",
     "iopub.status.idle": "2020-11-20T09:43:07.722965Z",
     "shell.execute_reply": "2020-11-20T09:43:07.723395Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "Prepare to save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:07.913496Z",
     "iopub.status.busy": "2020-11-20T09:43:07.913137Z",
     "iopub.status.idle": "2020-11-20T09:43:07.925732Z",
     "shell.execute_reply": "2020-11-20T09:43:07.925415Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "msg = 'Starting data saving process...'\n",
    "logging.info(msg)\n",
    "\n",
    "# create dir\n",
    "outdir = p['output_data_dir']\n",
    "if os.path.exists(outdir):\n",
    "    msg = 'Warning! Directory {} already exist! Deleting...\\n'.format(outdir)\n",
    "    logging.info(msg)\n",
    "    print(msg)\n",
    "    try:\n",
    "        shutil.rmtree(outdir)\n",
    "    except OSError as e:\n",
    "        msg  = 'Dir {} could not be deleted!\\n\\nOSError: {}'.format(outdir, e)\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "\n",
    "msg = 'Creating dir: {}'.format(outdir)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "os.makedirs(outdir, exist_ok=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:07.930569Z",
     "iopub.status.busy": "2020-11-20T09:43:07.930127Z",
     "iopub.status.idle": "2020-11-20T09:43:07.931742Z",
     "shell.execute_reply": "2020-11-20T09:43:07.932100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get channels ids (proteins) which will be used to predict transcripcion rate\n",
    "input_ids = list(data['val'][0].channels.set_index('name').loc[p['input_channels']]['channel_id'])\n",
    "# Get id of the channel that measure trancripcion rate\n",
    "output_ids = list(data['val'][0].channels.set_index('name').loc[p['output_channels']]['channel_id'])\n",
    "# add output channel id after the input channels ids\n",
    "channels_ids = input_ids + output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:07.936849Z",
     "iopub.status.busy": "2020-11-20T09:43:07.936455Z",
     "iopub.status.idle": "2020-11-20T09:43:43.368591Z",
     "shell.execute_reply": "2020-11-20T09:43:43.368202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note! instead of calculating the response value (y) here and save\n",
    "# it separatelly, we will do it on the modeling part\n",
    "\n",
    "if p['convert_into_image']: \n",
    "    msg = 'Saving images and masks...'\n",
    "    logging.info(msg)\n",
    "    \n",
    "    output_files = normalize_and_save(\n",
    "        mppdata_dict=data,\n",
    "        norm_vals=np.array(p['normalise_rescale_values']),\n",
    "        channels_ids=channels_ids,\n",
    "        outdir=outdir\n",
    "    )\n",
    "    p['output_files'] = output_files\n",
    "    print(output_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save metadata and used parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T09:43:43.373151Z",
     "iopub.status.busy": "2020-11-20T09:43:43.372783Z",
     "iopub.status.idle": "2020-11-20T09:43:43.450309Z",
     "shell.execute_reply": "2020-11-20T09:43:43.449939Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Saving Parameters and Metadata...'\n",
    "logging.info(msg)\n",
    "\n",
    "# save params\n",
    "json.dump(p, open(os.path.join(outdir, 'params.json'), 'w'), indent=4)\n",
    "\n",
    "# save metadata\n",
    "metadata.to_csv(os.path.join(outdir, 'metadata.csv'))\n",
    "\n",
    "# Save used channels\n",
    "#train.channels.to_csv(os.path.join(outdir, 'channels.csv'))\n",
    "k=list(data.keys())[0]\n",
    "data[k][0].channels.set_index('channel_id').loc[channels_ids].to_csv(os.path.join(outdir, 'channels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
