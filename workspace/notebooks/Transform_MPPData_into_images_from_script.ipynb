{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing to Transforming MPP data into images and predicting Transcription Rate (TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ment to convert raw cell data from several wells into multichannel images (along with its corresponding metadata).\n",
    "\n",
    "Data was taken from:\n",
    "`/storage/groups/ml01/datasets/raw/20201020_Pelkmans_NascentRNA_hannah.spitzer/` and server `vicb-submit-01`. \n",
    "\n",
    "In the preprocessing done in this notebook, NO discrimination of channels is done! All the channels are saved in the same order and all of them are also projected into a scalars, in case another channel wants to be used as target varaible. The objective of this preprocessing is to create a 'imaged' version of the MPP data.\n",
    "\n",
    "The discretization of the channels (input_channels) and the selection of the target variable is done during the convertion into tensorflow dataset!\n",
    "\n",
    "Considerations:\n",
    "- The MPPData saved in a dictionary and is never merged (only the metadata)! This is because there is not 'in place' option to extend numpy arrays (they need contiguous space in the memory). Therefore, any merging over numpy arrays results into object duplication in memory during the merging process.\n",
    "- The images are saved as variables in each MPPData instance as arrays of dtype=np.uint16. This saves a lot of ram memory during the processing. However, this only allows values between 0 and 65535 (which is the measure range of MPP data). Therefore, the normalization is done image by image during the saving into disk, again to reduce the use of ram memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:56.521377Z",
     "iopub.status.busy": "2020-11-22T17:19:56.520545Z",
     "iopub.status.idle": "2020-11-22T17:19:57.014176Z",
     "shell.execute_reply": "2020-11-22T17:19:57.013711Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# To display all the columns\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:57.018883Z",
     "iopub.status.busy": "2020-11-22T17:19:57.018441Z",
     "iopub.status.idle": "2020-11-22T17:19:57.020643Z",
     "shell.execute_reply": "2020-11-22T17:19:57.020935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "if not os.path.exists(PARAMETERS_FILE):\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "    \n",
    "# Open parameters\n",
    "with open(PARAMETERS_FILE) as params_file:\n",
    "    p = json.load(params_file)\n",
    "p.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into the loaded parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:57.024831Z",
     "iopub.status.busy": "2020-11-22T17:19:57.024250Z",
     "iopub.status.idle": "2020-11-22T17:19:57.026794Z",
     "shell.execute_reply": "2020-11-22T17:19:57.026488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths and Load external libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:57.031528Z",
     "iopub.status.busy": "2020-11-22T17:19:57.031096Z",
     "iopub.status.idle": "2020-11-22T17:19:58.378361Z",
     "shell.execute_reply": "2020-11-22T17:19:58.377901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data path\n",
    "DATA_DIR = p['raw_data_dir']\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise Exception('Data path {} does not exist!'.format(DATA_DIR))\n",
    "else:\n",
    "    print('DATA_DIR: {}'.format(DATA_DIR))\n",
    "\n",
    "# Load external libraries path\n",
    "EXTERNAL_LIBS_PATH = p['external_libs_path']\n",
    "if not os.path.exists(EXTERNAL_LIBS_PATH):\n",
    "    raise Exception('External library path {} does not exist!'.format(EXTERNAL_LIBS_PATH))\n",
    "else:\n",
    "    print('EXTERNAL_LIBS_PATH: {}'.format(EXTERNAL_LIBS_PATH))\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, EXTERNAL_LIBS_PATH)\n",
    "# Load external libraries\n",
    "from pelkmans.mpp_data import MPPData as MPPData\n",
    "from pelkmans.mpp_data import save_to_file_targets_masks_and_normalized_images as normalize_and_save\n",
    "from pelkmans.mpp_data import get_image_normalization_vals as get_normalization_vals\n",
    "from pelkmans.mpp_data import get_concatenated_metadata as get_concatenated_metadata\n",
    "\n",
    "# Set logging configuration\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename=p['log_file'],\n",
    "    filemode='w', \n",
    "    level=getattr(logging, p['log_level'])\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available data (Perturbations and Wells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:58.382553Z",
     "iopub.status.busy": "2020-11-22T17:19:58.382135Z",
     "iopub.status.idle": "2020-11-22T17:19:58.384584Z",
     "shell.execute_reply": "2020-11-22T17:19:58.384205Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Reading local available perturbations-wells...')\n",
    "# Save available local Perturbations and Wells\n",
    "perturbations = [per for per in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, per))]\n",
    "local_data = {}\n",
    "#print('Local available perturbations-wells:\\n')\n",
    "for per in perturbations:\n",
    "    pertur_dir = os.path.join(DATA_DIR, per)\n",
    "    wells = [w for w in os.listdir(pertur_dir) if os.path.isdir(os.path.join(pertur_dir, w))]\n",
    "    #print('{}\\n\\t{}\\n'.format(p, wells))\n",
    "    local_data[per] = wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Perturbations and its wells to process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:58.390242Z",
     "iopub.status.busy": "2020-11-22T17:19:58.389838Z",
     "iopub.status.idle": "2020-11-22T17:19:58.392128Z",
     "shell.execute_reply": "2020-11-22T17:19:58.391746Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Local available perturbations-wells:\\n{}'.format(local_data)\n",
    "print(msg)\n",
    "logging.debug(msg)\n",
    "\n",
    "# In case you only want to load some specific perturbations and/or wells here:\n",
    "#selected_data = {\n",
    "#    '184A1_hannah_unperturbed': ['I11', 'I09'],\n",
    "#    '184A1_hannah_TSA': ['J20', 'I16'],\n",
    "#}\n",
    "\n",
    "# Load perturbations-wells from parameters file\n",
    "selected_data = p['perturbations_and_wells']\n",
    "# How many wlls will be processed?\n",
    "n_wells = 0\n",
    "for key in list(selected_data.keys()):\n",
    "    n_wells += len(selected_data[key])\n",
    "\n",
    "print('\\nSelected perturbations-wells:\\n{}'.format(selected_data))\n",
    "\n",
    "#Generate and save data dirs\n",
    "data_dirs = []\n",
    "for per in selected_data.keys():\n",
    "    for w in selected_data[per]:\n",
    "        d = os.path.join(DATA_DIR, per, w)\n",
    "        data_dirs.append(d)\n",
    "        if not os.path.exists(d):\n",
    "            msg = '{} does not exist!\\nCheck if selected_data contain elements only from local_data dict.'.format(d)\n",
    "            logging.error(msg)\n",
    "            raise Exception(msg)\n",
    "p['data_dirs'] = data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:19:58.402604Z",
     "iopub.status.busy": "2020-11-22T17:19:58.401704Z",
     "iopub.status.idle": "2020-11-22T17:21:31.089556Z",
     "shell.execute_reply": "2020-11-22T17:21:31.089162Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Starting processing of {} wells...'.format(n_wells)\n",
    "logging.info(msg)\n",
    "\n",
    "data = {\n",
    "    'train':[],\n",
    "    'val':[],\n",
    "    'test':[]\n",
    "}\n",
    "\n",
    "for w, data_dir in enumerate(p['data_dirs'], 1):\n",
    "    msg = 'Processing well {}/{} from dir {}...'.format(w, n_wells, data_dir)\n",
    "    logging.info(msg)\n",
    "    print('\\n\\n'+msg)\n",
    "    # Load data as an MPPData object\n",
    "    mpp_temp = MPPData.from_data_dir(data_dir,\n",
    "                                     dir_type=p['dir_type'],\n",
    "                                     seed=p['seed'])\n",
    "    \n",
    "    # Add cell cycle to metadata (G1, S, G2)\n",
    "    # Important! If mapobject_id_cell is not in cell_cycle_file =>\n",
    "    # its corresponding cell is in Mitosis phase!\n",
    "    if p['add_cell_cycle_to_metadata']:\n",
    "        msg = 'Adding cell cycle to metadata...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.add_cell_cycle_to_metadata(os.path.join(DATA_DIR, p['cell_cycle_file']))\n",
    "    \n",
    "    # Add well info to metadata\n",
    "    if p['add_well_info_to_metadata']:\n",
    "        msg = 'Adding well info to metadata...'\n",
    "        logging.info('Adding well info to metadata...')\n",
    "        print(msg)\n",
    "        mpp_temp.add_well_info_to_metadata(os.path.join(DATA_DIR, p['well_info_file']))\n",
    "    \n",
    "    # Remove unwanted cells\n",
    "    if p.get('filter_criteria', None) is not None:\n",
    "        msg = 'Removing unwanted cells...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.filter_cells(p['filter_criteria'], p['filter_values'])\n",
    "\n",
    "    # Subtract background  values for each channel\n",
    "    if p['subtract_background']:\n",
    "        print('Subtracting background...')\n",
    "        mpp_temp.subtract_background(os.path.join(DATA_DIR, p['background_value']))\n",
    "    \n",
    "    # Project every uni-channel images into a scalar for further analysis\n",
    "    if p['project_into_scalar']:\n",
    "        msg = 'Projecting data...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        mpp_temp.add_scalar_projection(p['method'])\n",
    "        \n",
    "    # Split data into train, validation and test\n",
    "    msg = 'Spliting data into train, validation and test'\n",
    "    logging.info(msg)\n",
    "    print(msg)\n",
    "    train_temp, val_temp, test_temp = mpp_temp.train_val_test_split(p['train_frac'], p['val_frac'])\n",
    "    del(mpp_temp)\n",
    "    \n",
    "    if p['convert_into_image']:\n",
    "        msg = 'Converting data into images...'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        train_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Train dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        val_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Validation dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "        test_temp.add_image_and_mask(data='MPP', remove_original_data=p['remove_original_data'], img_size=p['img_size'])\n",
    "        msg = 'Test dataset converted'\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "    \n",
    "    # Validate same channels across wells\n",
    "    if len(data['val']) > 0:\n",
    "        if not all(data['val'][0].channels.name == val_temp.channels.name):\n",
    "            raise Exception('Channels across MPPData instances are not the same!')\n",
    "            \n",
    "    data['val'].append(val_temp)\n",
    "    del(val_temp)\n",
    "    data['test'].append(test_temp)\n",
    "    del(test_temp)\n",
    "    data['train'].append(train_temp)\n",
    "    del(train_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During this preprocessing we save all channels!\n",
    "channels_ids = data['val'][0].channels.channel_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get normalization values from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:21:31.093048Z",
     "iopub.status.busy": "2020-11-22T17:21:31.092683Z",
     "iopub.status.idle": "2020-11-22T17:21:38.154082Z",
     "shell.execute_reply": "2020-11-22T17:21:38.153694Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalization values got from the train data (inner percentile% of train data)\n",
    "if p['normalise']:\n",
    "    msg = 'Normalizing data...'\n",
    "    logging.info(msg)\n",
    "    if p['convert_into_image']:\n",
    "        # Onle get the normalization parameters here and normalize \n",
    "        # images during saving into files\n",
    "        rescale_values = get_normalization_vals(\n",
    "            instance_dict=data['train'],\n",
    "            input_channel_ids=channels_ids,\n",
    "            percentile=p['percentile'])\n",
    "    if not p['remove_original_data']:\n",
    "        # Get normalization values and normalize original MPPData\n",
    "        pass\n",
    "        # TODO: Rewrite rescale_intensities_per_channel to deal with\n",
    "        # a dictionary of MPPData instances, instead of a big numpy array\n",
    "        # (this is to avoid the duplication in memory)\n",
    "        #rescale_values = train.rescale_intensities_per_channel(percentile=p['percentile'], )\n",
    "        #_ = val.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "        #_ = test.rescale_intensities_per_channel(rescale_values=rescale_values)\n",
    "    p['normalise_rescale_values'] = list(rescale_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extra, all channels are projected into an scalar and saved in the metadata. The idea of this is to have scalar data to make simpler analysis. This projected data is also normalized using the train projection values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:21:38.164238Z",
     "iopub.status.busy": "2020-11-22T17:21:38.163314Z",
     "iopub.status.idle": "2020-11-22T17:21:38.194360Z",
     "shell.execute_reply": "2020-11-22T17:21:38.194726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge metadata and Normalize projected values\n",
    "msg = 'Normalizing Projected data...'\n",
    "logging.info(msg)\n",
    "    \n",
    "metadata, rescale_values = get_concatenated_metadata(\n",
    "    mppdata_dict=data,\n",
    "    normalize=(p['normalise'] & p['project_into_scalar']),\n",
    "    norm_key='train',\n",
    "    projection_method=p['method'],\n",
    "    percentile=p['percentile'])\n",
    "\n",
    "p['normalise_rescale_values_scalars'] = list(rescale_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:21:38.220543Z",
     "iopub.status.busy": "2020-11-22T17:21:38.216911Z",
     "iopub.status.idle": "2020-11-22T17:21:38.244761Z",
     "shell.execute_reply": "2020-11-22T17:21:38.244383Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "Prepare to save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:21:38.249224Z",
     "iopub.status.busy": "2020-11-22T17:21:38.248846Z",
     "iopub.status.idle": "2020-11-22T17:21:38.965453Z",
     "shell.execute_reply": "2020-11-22T17:21:38.965048Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "msg = 'Starting data saving process...'\n",
    "logging.info(msg)\n",
    "\n",
    "# create dir\n",
    "outdir = p['output_data_dir']\n",
    "if os.path.exists(outdir):\n",
    "    msg = 'Warning! Directory {} already exist! Deleting...\\n'.format(outdir)\n",
    "    logging.info(msg)\n",
    "    print(msg)\n",
    "    try:\n",
    "        shutil.rmtree(outdir)\n",
    "    except OSError as e:\n",
    "        msg  = 'Dir {} could not be deleted!\\n\\nOSError: {}'.format(outdir, e)\n",
    "        logging.info(msg)\n",
    "        print(msg)\n",
    "\n",
    "msg = 'Creating dir: {}'.format(outdir)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "os.makedirs(outdir, exist_ok=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Images, masks and targets of all channels into separated files using the mapobject_id_cell of each cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p['convert_into_image']: \n",
    "    msg = 'Saving images and masks...'\n",
    "    logging.info(msg)\n",
    "    \n",
    "    output_files = normalize_and_save(\n",
    "        mppdata_dict=data,\n",
    "        norm_vals=np.array(p['normalise_rescale_values']),\n",
    "        channels_ids=channels_ids,\n",
    "        projection_method=p['method'],\n",
    "        outdir=outdir\n",
    "    )\n",
    "    p['output_files'] = output_files\n",
    "    print(output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save metadata and used parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T17:22:24.507083Z",
     "iopub.status.busy": "2020-11-22T17:22:24.506424Z",
     "iopub.status.idle": "2020-11-22T17:22:24.808514Z",
     "shell.execute_reply": "2020-11-22T17:22:24.808887Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Saving Parameters and Metadata...'\n",
    "logging.info(msg)\n",
    "\n",
    "# save params\n",
    "json.dump(p, open(os.path.join(outdir, 'params.json'), 'w'), indent=4)\n",
    "\n",
    "# save metadata\n",
    "metadata.to_csv(os.path.join(outdir, 'metadata.csv'))\n",
    "\n",
    "# Save used channels\n",
    "#train.channels\n",
    "data['train'][0].channels.to_csv(os.path.join(outdir, 'channels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load one saved file and take a look into the content to see if everithing was done correctlly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = np.random.choice(metadata['mapobject_id_cell'].values)\n",
    "cell_subset = metadata.set[metadata.mapobject_id_cell == cell_id].values[0]\n",
    "file = os.path.join(outdir, cell_subset,str(cell_id)+'.npz')\n",
    "cell = np.load(file)\n",
    "cell_img = cell['img']\n",
    "cell_mask = cell['mask']\n",
    "cell_targets = cell['targets']\n",
    "\n",
    "print('Cell image shape: {}\\n'.format(cell_img.shape))\n",
    "print('Cell mask shape: {}\\n'.format(cell_mask.shape))\n",
    "print('Cell target shape: {}\\n'.format(cell_targets.shape))\n",
    "print('Cell targets: {}\\n'.format(cell_targets))\n",
    "\n",
    "# Now take a look into its image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cell_img[:,:,0:3],\n",
    "            cmap=plt.cm.PiYG,\n",
    "            vmin=0, vmax=1\n",
    "          )\n",
    "plt.show()\n",
    "logging.info('\\n\\nPREPROCESSING FINISHED!!!!----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
