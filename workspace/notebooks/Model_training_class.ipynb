{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "The objective of this notebook is train and evaluate a given model specified in the parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development and debugging:\n",
    "# Reload modul without restarting the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import socket\n",
    "\n",
    "# Set terminal output (to send mesages to the terminal stdout)\n",
    "terminal_output = open('/dev/stdout', 'w')\n",
    "print('Execution of Notebook started at {}'.format(datetime.now()), file=terminal_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external libraries\n",
    "key = 'external_libs_path'\n",
    "if socket.gethostname() == 'hughes-machine':\n",
    "    external_libs_path = '/home/hhughes/Documents/Master_Thesis/Project/workspace/libs'\n",
    "else:\n",
    "    external_libs_path= '/storage/groups/ml01/code/andres.becker/master_thesis/workspace/libs'\n",
    "print('External libs path: \\n'+external_libs_path, file=terminal_output)\n",
    "\n",
    "if not os.path.exists(external_libs_path):\n",
    "    msg = 'External library path {} does not exist!'.format(external_libs_path)\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Add EXTERNAL_LIBS_PATH to sys paths (for loading libraries)\n",
    "sys.path.insert(1, external_libs_path)\n",
    "# Load external libraries\n",
    "#from Utils import lr_schedule_Callback\n",
    "#from Utils import save_best_model_Callback\n",
    "import Utils as utils\n",
    "from Utils import evaluate_model\n",
    "import Data_augmentation\n",
    "from Models_V2 import Individual_Model_Training\n",
    "# load function to print in the here and in the log file at the same time\n",
    "from Utils import print_stdout_and_log as printc\n",
    "from Utils import set_GPU_config as set_GPU_config\n",
    "import tfds_utils\n",
    "\n",
    "from Costum_Callbacks import set_tensorboard as set_tensorboard_CB\n",
    "from Costum_Callbacks import save_best_model_base_on_CMA_Callback as CMA_CB\n",
    "#from Costum_Callbacks import save_best_model_weights_in_memory_Callback as save_w_and_b\n",
    "from Costum_Callbacks import print_progress_to_log as train_progress_to_log_CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not touch the value of PARAMETERS_FILE!\n",
    "# When this notebook is executed with jupyter-nbconvert (from script), \n",
    "# it will be replaced outomatically\n",
    "#PARAMETERS_FILE = '/home/hhughes/Documents/Master_Thesis/Project/workspace/scripts/Parameters/model_params/local/Quick_test_local.json'\n",
    "PARAMETERS_FILE = 'dont_touch_me-input_parameters_file'\n",
    "\n",
    "# Open parameters\n",
    "if os.path.exists(PARAMETERS_FILE):\n",
    "    with open(PARAMETERS_FILE) as file:\n",
    "        p = json.load(file)\n",
    "else:\n",
    "    raise Exception('Parameter file {} does not exist!'.format(PARAMETERS_FILE))\n",
    "\n",
    "# IMPORTANT\n",
    "# All outputs are saved using the model name and the name of the parameters file\n",
    "# For instance, if model='ResNet50V2' and param file='test_1.json', then\n",
    "# the model will be saved saved at p['model_path']/ResNet50V2/test_1\n",
    "\n",
    "# Save parameter file path\n",
    "p['parameters_file_path'] = PARAMETERS_FILE\n",
    "p['external_libs_path'] = external_libs_path\n",
    "# check and set default parameters\n",
    "p, info = utils.set_model_default_parameters(p)\n",
    "print(info)\n",
    "print(p.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=p['log_file'],\n",
    "    filemode='w', \n",
    "    level=getattr(logging, 'INFO')\n",
    ")\n",
    "logging.info('Parameters loaded from file:\\n{}'.format(PARAMETERS_FILE))\n",
    "msg = 'Log file: '+p['log_file']\n",
    "# print selected parameters into the log\n",
    "logging.info(info)\n",
    "# Print location of the log file into the terminal\n",
    "print(msg, file=terminal_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU config\n",
    "set_GPU_config(p['disable_gpu'], p['set_memory_growth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dirs where model output will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to avoid cleaning (deleting) model dir, then uncomment the next line:\n",
    "#p['clean_model_dir'] = 0\n",
    "\n",
    "base_path, model_path, checkpoints_path = utils.create_model_dirs(parameters=p)\n",
    "\n",
    "msg = 'Base path:\\n{}'.format(base_path)\n",
    "msg += '\\nModel path:\\n{}'.format(model_path)\n",
    "msg += '\\nCheckpoints path:\\n{}'.format(checkpoints_path)\n",
    "printc(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.- Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where tf datasets are\n",
    "dataset, ds_info = tfds.load(\n",
    "    name=p['tf_ds_name'], \n",
    "    data_dir=p['local_tf_datasets'], \n",
    "    # If False, returns a dictionary with all the features\n",
    "    as_supervised=True, \n",
    "    shuffle_files=p['shuffle_files'],\n",
    "    with_info=True)\n",
    "\n",
    "# Load splits\n",
    "train_data, val_data = dataset['train'], dataset['validation']\n",
    "\n",
    "msg = 'Tensorflow dataset {} loaded from:\\n{}'.format(p['tf_ds_name'], p['local_tf_datasets'])\n",
    "printc(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFDS metadata\n",
    "tfds_metadata = tfds_utils.Costum_TFDS_metadata().load_metadata(ds_info.data_dir)\n",
    "tfds_metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_metadata['channels_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_metadata['metadata_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.- Data preprocessing, data augmentation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the network, we discriminate some channels, apply some linear transformations (90deg rotations and horizontal flipping) to augment the **Training** dataset, create the batches and shuffle them. Also, we perform other operations to improve performance.\n",
    "\n",
    "**Tune performance**<br>\n",
    "tf.data.Dataset.prefetch overlaps data preprocessing and model execution while training.\n",
    "It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to **tf.data.experimental.AUTOTUNE** which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
    "\n",
    "**Shuffling**<br>\n",
    "dataset.shuffle() Randomly shuffles the elements of this dataset.\n",
    "This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n",
    "\n",
    "**reshuffle_each_iteration** controls whether the shuffle order should be different for each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Channels file\n",
    "selected_channels = p['input_channels']\n",
    "msg = 'Selected input channels:\\n{}'.format(selected_channels)\n",
    "logging.info(msg)\n",
    "print(msg)\n",
    "\n",
    "# Get selected channel ids\n",
    "input_ids = np.array(tfds_metadata['channels_df'].set_index(['name']).loc[selected_channels].TFDS_channel_id.values)\n",
    "input_ids = input_ids.astype(np.int16)\n",
    "msg = 'Corresponding input channel ids:\\n{}'.format(input_ids)\n",
    "printc(msg)\n",
    "printc('\\nNumber of input channels to use in the model:\\n{}'.format(input_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing and data augemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cell size is sampled from a normal distribution, calculate the parameters \n",
    "# of the distribution using the train set\n",
    "if (p['CenterZoom_mode'] == 'random_normal') and p['CenterZoom']:\n",
    "    \n",
    "    mask = (tfds_metadata['metadata_df'].set == 'train')\n",
    "    p['cell_size_ratio_mean'] = tfds_metadata['metadata_df'][mask].cell_size_ratio.mean()\n",
    "    p['cell_size_ratio_stddev'] = tfds_metadata['metadata_df'][mask].cell_size_ratio.std()\n",
    "    p['cell_size_ratio_low_bound'] = tfds_metadata['metadata_df'][mask].cell_size_ratio.min()\n",
    "    msg = '\\nrandom_normal selected as CenterZoom_mode. Distribution parameters:'\n",
    "    msg += '\\nmean: {}, stddev: {}, lower bound: {}'.format(p['cell_size_ratio_mean'], p['cell_size_ratio_stddev'], p['cell_size_ratio_low_bound'])\n",
    "    printc(msg)\n",
    "    \n",
    "    hue_order = ['G1', 'S', 'G2']\n",
    "    sns.kdeplot(data=tfds_metadata['metadata_df'][mask],\n",
    "            x='cell_size_ratio',\n",
    "            hue='cell_cycle',\n",
    "            hue_order=hue_order,\n",
    "            shade=True,\n",
    "            bw_method=0.2\n",
    "           )\n",
    "    plt.title('cell_size_ratio distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look into one image and the selected data augmentation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch to print a cell\n",
    "images, targets = next(iter(val_data))\n",
    "\n",
    "# add an extra dim (like a batch)\n",
    "image = tf.expand_dims(images, axis=0)\n",
    "Data_augmentation.visualize_data_augmentation(image, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some necessary stuff for data preprocessing (projection tensor to filter the input channels)\n",
    "input_shape = np.array(ds_info.features['image'].shape)\n",
    "projection_tensor = Data_augmentation.get_projection_tensor(input_shape, input_ids)\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "train_data, val_data = Data_augmentation.prepare_train_and_val_TFDS(train_data,\n",
    "                                                                    val_data,\n",
    "                                                                    projection_tensor,\n",
    "                                                                    p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Model Selection\n",
    "\n",
    "Models are selected from a group of predefined models in the class `Predef_models` (in `Models.py`). The name of the selected model is specified in the parameter `p['model_method']`.\n",
    "\n",
    "First we need to init the `Predef_models` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = ds_info.features['image'].shape[:-1] + (input_ids.shape[0],)\n",
    "\n",
    "# init model class\n",
    "temp_run = Individual_Model_Training()\n",
    "\n",
    "# init model architectur\n",
    "temp_run.init_model(arch_name=p['model_name'],\n",
    "                    input_shape=img_shape,\n",
    "                    conv_reg=p['conv_reg'],\n",
    "                    dense_reg=p['dense_reg'],\n",
    "                    bias_l2_reg=p['bias_l2_reg'],\n",
    "                    pre_training=p['pre_training'],\n",
    "                    return_custom_model=p['custom_model_class']\n",
    "                   )\n",
    "\n",
    "# select loss function and build the model\n",
    "temp_run.build_model(loss_name=p['loss'], learning_rate=p['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to print train progren in log file\n",
    "train_progress_to_log = train_progress_to_log_CB(p['number_of_epochs'])\n",
    "temp_run.callbacks.append(train_progress_to_log)\n",
    "\n",
    "# tensorboard callbacks\n",
    "if p['tensorboard']:\n",
    "    tensorboard = set_tensorboard_CB(log_path=p['log_path'], log_dir_name=p['model_name'])\n",
    "    temp_run.callbacks.append(tensorboard)\n",
    "\n",
    "# set Center moving average callback\n",
    "avg_sizes = [11, 21, 31]\n",
    "monitor='val_mean_absolute_error'\n",
    "loss_CMA = CMA_CB(monitor, avg_sizes)\n",
    "temp_run.callbacks.append(loss_CMA)\n",
    "\n",
    "# Early stop\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', \n",
    "                                              mode='min', \n",
    "                                              patience=p['early_stop_patience'],\n",
    "                                              min_delta=0,\n",
    "                                              restore_best_weights=False,\n",
    "                                              verbose=1\n",
    "                                             )\n",
    "temp_run.callbacks.append(early_stop)\n",
    "\n",
    "# print loaded callbacks\n",
    "msg = 'Loaded callbacks:\\n{}'.format(temp_run.callbacks)\n",
    "printc(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_run.fit_model(train_data, val_data, p['number_of_epochs'], p['verbose_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename some vars to make it easier\n",
    "history = temp_run.history.history\n",
    "metrics = temp_run.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_metrics(history=history, metrics=['loss']+metrics, p=p, figsize=(15,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Saiving trained model'\n",
    "logging.info(msg)\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(base_path, 'history.json'), 'w') as file:\n",
    "    json.dump(history, file, indent=4)\n",
    "    \n",
    "# Save CMA history\n",
    "# First wee need to convert from np.int64 and np.float64 to regular python int and float\n",
    "temp_dict = {}\n",
    "for key in loss_CMA.CMA_history.keys():\n",
    "    temp_dict[key] = [[int(item[0]), float(item[1])] for item in loss_CMA.CMA_history[key]]\n",
    "with open(os.path.join(base_path, 'CMA_history.json'), 'w') as file:\n",
    "    json.dump(temp_dict, file, indent=4)\n",
    "    \n",
    "# Save parameters\n",
    "with open(os.path.join(base_path, 'parameters.json'), 'w') as file:\n",
    "    json.dump(p, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history\n",
    "#path = ''\n",
    "#with open(os.path.join(path, 'history.json'), 'r') as file:\n",
    "#    history = json.load(file)\n",
    "# Save parameters\n",
    "#with open(os.path.join(base_path, 'parameters.json'), 'r') as file:\n",
    "#    p = json.load(file)\n",
    "#metrics = ['mse', 'mean_absolute_error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame to save model metrics\n",
    "metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, temp_run.model, projection_tensor, tfds_metadata['metadata_df'])\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics()\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Best model with no Center Moving Average (CMA_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_0'\n",
    "# Load weights\n",
    "temp_run.model.set_weights(loss_CMA.best_models[eval_name][3])\n",
    "# Save model\n",
    "temp_run.model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, temp_run.model, projection_tensor, tfds_metadata['metadata_df'])\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history, \n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=0, \n",
    "                       CMA=loss_CMA.best_models[eval_name][1], \n",
    "                       CMA_Std=loss_CMA.best_models[eval_name][2], \n",
    "                       Epoch=loss_CMA.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Best model wrt Central Moving Average of size 11 (CMA_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_11'\n",
    "# Load weights\n",
    "temp_run.model.set_weights(loss_CMA.best_models[eval_name][3])\n",
    "# Save model\n",
    "temp_run.model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, temp_run.model, projection_tensor, tfds_metadata['metadata_df'])\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history, \n",
    "                         CMA_history=loss_CMA.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=11, \n",
    "                       CMA=loss_CMA.best_models[eval_name][1], \n",
    "                       CMA_Std=loss_CMA.best_models[eval_name][2], \n",
    "                       Epoch=loss_CMA.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Best model wrt Central Moving Average of size 21 (CMA_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_21'\n",
    "# Load weights\n",
    "temp_run.model.set_weights(loss_CMA.best_models[eval_name][3])\n",
    "# Save model\n",
    "temp_run.model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, temp_run.model, projection_tensor, tfds_metadata['metadata_df'])\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history, \n",
    "                         CMA_history=loss_CMA.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=21, \n",
    "                       CMA=loss_CMA.best_models[eval_name][1], \n",
    "                       CMA_Std=loss_CMA.best_models[eval_name][2], \n",
    "                       Epoch=loss_CMA.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Best model wrt Central Moving Average of size 31 (CMA_31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name='CMA_31'\n",
    "# Load weights\n",
    "temp_run.model.set_weights(loss_CMA.best_models[eval_name][3])\n",
    "# Save model\n",
    "temp_run.model.save(os.path.join(model_path, eval_name))\n",
    "# Evaluate model\n",
    "model_eval = evaluate_model(p, temp_run.model, projection_tensor, tfds_metadata['metadata_df'])\n",
    "# Save model data (y_hat values and metrics)\n",
    "model_eval.save_model_evaluation_data(base_path, eval_name=eval_name)\n",
    "model_eval.targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "utils.plot_train_metrics(history=history, \n",
    "                         CMA_history=loss_CMA.CMA_history[eval_name],\n",
    "                         CMA_metric='mean_absolute_error',\n",
    "                         metrics=['mean_absolute_error'], \n",
    "                         p=p,\n",
    "                         title=eval_name,\n",
    "                         figsize=(15,23))\n",
    "# Error distribution plot\n",
    "sets = ['train','val']\n",
    "model_eval.plot_error_dist(figsize=(18,6), sets=sets)\n",
    "# y and y_hat distribution plot\n",
    "model_eval.plot_y_dist(figsize=(15,7), sets=sets)\n",
    "# Residuals plot\n",
    "model_eval.plot_residuals(figsize=(10,7), sets=sets)\n",
    "# Target vs predicted\n",
    "model_eval.plot_y_vs_y_hat(figsize=(7.6,7), sets=sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval.get_metrics(CMA_size=31, \n",
    "                       CMA=loss_CMA.best_models[eval_name][1], \n",
    "                       CMA_Std=loss_CMA.best_models[eval_name][2], \n",
    "                       Epoch=loss_CMA.best_models[eval_name][0])\n",
    "metrics_df = metrics_df.append(model_eval.metrics_df, ignore_index=True)\n",
    "mask = model_eval.metrics_df.Set == 'test'\n",
    "model_eval.metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'test'\n",
    "metrics_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metrics_df.Set == 'val'\n",
    "metrics_df[mask].sort_values(by=['MAE', 'Bias', 'Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "with open(os.path.join(base_path, 'metrics.csv'), 'w') as file:\n",
    "    metrics_df.to_csv(file, index=False)\n",
    "\n",
    "# Save a copy into a common dir to comparation among models\n",
    "temp_path = os.path.join(p['model_path'], 'Model_Metrics_RI_2')\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "with open(os.path.join(temp_path, p['basename']+'.csv'), 'w') as file:\n",
    "    metrics_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'Notebook execution finished!'\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
