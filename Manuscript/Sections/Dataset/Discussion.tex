%% Magic command to compile root document
% !TEX root = ../../thesis.tex
% reset glossary to show full acrs
\glsresetall
% define where the images are
\graphicspath{{./Sections/Dataset/Resources/}}

To identify nascent RNA inside a cell nucleus, the \gls{mpm} protocol use the 5-ethynyl uridine (EU) marker (which is then interpreted as \gls{tr}). However, it has been observed that this marker also binds to DNA molecules after some incubation time \cite{jao2008exploring}, \cite{bao2018capturing}. As future work, another \gls{mpm} dataset could be analyzed, either with a shorter or longer incubation time for the UE marker. Then, it would be interesting to validate if the results obtained with both datasets are consistent.

Besides the preprocessing techniques introduced in section \ref{sec:dataset:data_pp} (clipping and standardization), the following approaches were also tried

\begin{itemize}
  \item Linear scaling using the $98\%$ percentile with and without clipping.
  \item Mean extraction and linear scaling using the $98\%$ percentile with clipping (like standardization, but with the $98\%$ percentile instead of the standard deviation).
  \item $49\%$ percentile extraction and linear scaling using the $98\%$ percentile (no clipping).
\end{itemize}

\noindent This approaches were tried at a per-channel level. However, clipping plus standardization where the prprocessing techniques that showed the best performance. Since we seek the model to predict \gls{tr} base on spacial information, rather than pixel intensity/color, good performance means low \gls{mae} for the \gls{cnn} models, but high \gls{mae} for the linear model (since the linear model is unable to use the spatial information). This indicates that the spatial information encoded in the images of the data set has more influence on the prediction of the model than the information encoded in the colors.

Another aspect of the dataset that is worth to mention, is that more than half cells are in phase $G_1$ (see table \ref{table:data_pp:dataset_dist_cc}), while cells in $S$ phase are less than $30\%$ and around $15\%$ for $G_2$ cells. This causes the model to focus more on correctly predicting the \gls{tr} of $G_1$ cells, than for cells in the other two phases. This happens because $G_1$ cells have more influence on the minimization of the objective function, since it is more likely that the model is fed with $G_1$ cells during training.

As it is shown on figure \ref{fig:dataset:discus:tr_dist}, the \gls{tr} of $G_1$ cells is significantly lower than the \gls{tr} of $S$ and $G_2$ cells. This, and that cells in different phases are not in the same proportion in the dataset, could cause the model to make a biased prediction when it is fed with a $S$ or $G_2$ cell. Two possible solutions to this problem are, either to add more cells in phases $S$ and $G_2$ to the dataset, or to sample with replacement over the available cells, so the proportion of cells in the three different phases is the same in the dataset. Another possible solution would be to make a weight loss function based on the proportions of the cell phases, such that every phase has the same influence on it during training.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.7\linewidth]{TR_dist.jpg}
  \caption{\gls{tr} distribution separated by cell phase.}
  \label{fig:dataset:discus:tr_dist}
\end{figure}

In \cite{Smilkov_smoothgrad}, Smilkov et al. mention that the addition of random noise to the images during model training, can improve the quality of the score maps (less noisy score maps). For this reason, beside the data augmentation techniques introduced in this section, the addition of random noise was also implemented and tried. However, in practice this did not show any apparent improvement.
