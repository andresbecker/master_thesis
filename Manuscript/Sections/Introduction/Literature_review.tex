In recent years neural networks...

% Interpretability part
However, once we have successfully trained an artificial neural network to make predictions, one may also want to understand how the predictions was
made. Several approaches have being used to tackle this. In this work we focus in the ones that highlight features in an input that are most responsible for the model's output. We use a composition of two explainable AI techniques, \gls{ig} \cite{sundararajan2017axiomatic} to highlight relevant input components (saliency map) and \gls{vg} \cite{adebayo2020sanity} to enhance the empirical quality the saliency map.
