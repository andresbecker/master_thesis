%% Magic command to compile root document
% !TEX root = ../../thesis.tex

%% Reset glossary to show long gls names
%\glsresetall

\graphicspath{{./Sections/Methodology/Resources/}}

% experiment:
% You need to train the model!!

As we saw in section \ref{sec:basics:ANN}, each kernel in a regular convolution layer needs to simultaneously learn spatial and cross-channel correlations. For this reason, we also tested an architecture capable of separating these two tasks, the \hl{Xception} \cite{chollet2017xception}.

The Xception architecture combines the idea behind the Inception module and the residual blocks (see section \ref{sec:basics:ANN}).
We will not dive into details about the Xception here. However, the model architecture is shown in table \ref{table:metho:models:XC}.
The raw \hl{Xception feature extraction}, represent the feature extraction layers (i.e., all the layers containing convolution and/or pooling layers) of the Xception\footnote{For this work, we did not implement the Xception architecture from scratch, instead we used the pre-built model that is provided in the Keras library. For more information pleases refer to the \href{https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception}{official documentation}.}, while the remaining rows represent the layers intended to make the final prediction. The layers are evaluated from top to bottom. This model has $21,373,929$ free (learnable) parameters in total.

% set table lengths
\setlength{\mylinewidth}{\linewidth-7pt}%
\setlength{\mylengtha}{0.35\mylinewidth-2\arraycolsep}%
\setlength{\mylengthb}{0.25\mylinewidth-2\arraycolsep}%
\setlength{\mylengthc}{0.18\mylinewidth-2\arraycolsep}%

\begin{longtable}{m{\mylengtha} | m{\mylengthb} | m{\mylengthc}}
    \hline
    Layer & Output Shape & Number of parameters \\
    \hline
    Input & $(bs, 224, 224, 38)$ & 0 \\
    \hline
    Channel filtering & $(bs, 224, 224, 33)$ & 0 \\
    \hline
    Xception feature extraction & $(bs, 7, 7, 2048)$ & 20,814,824 \\
    \hline
    Global Average Pooling & $(bs, 2048)$ & 0 \\
    \hline
    Dense & $(bs, 256)$ & 524544 \\
    \hline
    Batch Normalization & $(bs, 256)$ & 1024 \\
    \hline
    ReLU & $(bs, 256)$ & 0 \\
    \hline
    Dense & $(bs, 128)$ & 32896 \\
    \hline
    Batch Normalization & $(bs, 128)$ & 512 \\
    \hline
    ReLU & $(bs, 128)$ & 0 \\
    \hline
    Dense & $(bs, 1)$ & 129 \\
    \hline
  \caption{Xception \gls{cnn} architecture. The rows represent each layer of the model. The flow of the model is from top to bottom. The $bs$ on the \hl{Output Shape} column stands for \hl{Batch size}.}
  \label{table:metho:models:XC}
\end{longtable}

The learning rate used to train the baseline model was $0.001$.
