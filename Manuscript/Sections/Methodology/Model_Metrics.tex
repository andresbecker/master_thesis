%% Magic command to compile root document
% !TEX root = ../../thesis.tex

%% Reset glossary to show long gls names
\glsresetall
\graphicspath{{./Sections/Methodology/Resources/}}

To evaluate and compare the performance of the models, besides the loss function (\hl{Huber loss}), we also used 2 other error measures

\begin{itemize}
  \item The \acrfull{mse}
    \begin{equation}
      E_{MSE}(Y,\hat{Y}) := \frac{1}{N} \sum_{n=1}^N (y_i-\hat{y}_i)^2
    \end{equation}
  \item The \acrfull{mae}
    \begin{equation}
      E_{MAE}(Y,\hat{Y}) := \frac{1}{N} \sum_{n=1}^N |y_i-\hat{y}_i|^2
    \end{equation}
\end{itemize}

\noindent where $Y, \hat{Y} \in \mathbb{R}^N$ are the true and predicted \gls{tr} values respectively.

Additionally, we also used the \hl{Coefficient of determination} $R^2$, which provides the proportion of the variance in the dependent variable $y$ that is explained by the model \cite{steel1960principles}

\begin{equation}
  %\begin{split}
    R^2 := 1 - \frac{SS_{res}}{SS_{tot}}
  %\end{split}
\end{equation}
\noindent where $SS_{res}:=\sum_{n=1}^N (y_i-\hat{y}_i)^2$ and $SS_{tot}:=\sum_{n=1}^N (y_i-\bar{y})^2$ are the \hl{Residual sum of squares} and the \hl{Total sum of squares} respectively.
